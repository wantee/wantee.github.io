<!DOCTYPE html>
<html class=" " lang="en" >
<head>
<meta charset="utf-8">
<meta name="author" content="Wantee Wang">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="http://wantee.github.io/2015/02/05/rnnlib-softmax-layer/" rel="canonical">
<meta name='generator' content='Jekyll v2.5.3, Octopress v3.0.0.rc.37 Octopress Ink v1.0.0.rc.64'>

<link href='/stylesheets/all-6811b0809930f663307ddbaaf4f331d2.css' media='all' rel='stylesheet' type='text/css'>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-58316654-2', 'auto');
    ga('send', 'pageview');
  </script>


</head>
<body>
  <div class="site"><div class='nav-panel'>
<nav class='mobile-nav' role="navigation">
  <div class='mobile-nav-item mobile-nav-search'><form class='site-search' action="https://google.com/search" method="get">
  <input type="hidden" name="sitesearch" value="wantee.github.io">
  <input class="search-input" type="text" name="q" results="0" placeholder="Search" accesskey="/">
</form></div>
  <a class="mobile-nav-item " href="/"/>Posts</a>

  
  
  
  
  <a class="mobile-nav-item " href="/archive/">Archive</a>
  
</nav>
</div>
<div class="site-content">
<div class="site-top"><div class="site-top-content">
<header role="banner" class="site-header">
  <h1 class="site-title"><a class="site-title-link" href="/"/>Formula Coding</a></h1>
</header>
<nav class='main-nav' role="navigation">
  <a class="main-nav-item " href="/"/>Posts</a>
  <a class="main-nav-item " href="/archive/">Archive</a>
  <div class='main-nav-item main-nav-search'><form class='site-search' action="https://google.com/search" method="get">
  <input type="hidden" name="sitesearch" value="wantee.github.io">
  <input class="search-input" type="text" name="q" results="0" placeholder="Search" accesskey="/">
</form></div>
  <button class="mobile-nav-toggle" href="#" onclick="(function(){ document.querySelector('html').classList.toggle('mobile-nav-active') })()"><span class="mobile-nav-icon"></span><span class="hidden-label">Navigate<span></span></span></button>
</nav>
</div></div>
<div class="site-main">
<div class="main ">
  <div class="main-content"><article class="post entry  " role="article">
  <header class="entry-header">
    
    <div class='entry-header-content'>
      <h1 class="entry-title">RNNLIB: Softmax&nbsp;Layer</h1>
      
      
    </div>
  </header>

  <div class="entry-meta"><a href="/2015/02/05/rnnlib-softmax-layer/"><time class='entry-date' datetime='2015-02-05T21:08:17+08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span></time></a><span class="printable"><a href="/assets/printables/2015-02-05-rnnlib-softmax-layer.pdf"> <img src="/images/octopress-printable/printer.png" alt="printable version"> </a></span><span class='entry-categories'><span class="category-links"><a class="category-link" href="/categories/neural network/">Neural network</a></span></span></div>

  <div class="entry-content"><ul id="markdown-toc">
  <li><a href="#fundamentals" id="markdown-toc-fundamentals">Fundamentals</a>    <ul>
      <li><a href="#list-of-symbols" id="markdown-toc-list-of-symbols">List of Symbols</a></li>
      <li><a href="#formulas" id="markdown-toc-formulas">Formulas</a></li>
      <li><a href="#layers-in-rnnlib" id="markdown-toc-layers-in-rnnlib">Layers in RNNLIB</a></li>
    </ul>
  </li>
  <li><a href="#forward-pass" id="markdown-toc-forward-pass">Forward Pass</a></li>
  <li><a href="#backpropagating" id="markdown-toc-backpropagating">Backpropagating</a></li>
</ul>

<p>I used to think that, in order to get the proper gradient, we have to take derivative of 
$\log$ of softmax with respect to weights. However,
the RNNLIB shows that we can actually factorize the network into single layers. In this post, 
we look into the Softmax Layer.</p>

<h2 id="fundamentals">Fundamentals</h2>

<h3 id="list-of-symbols">List of Symbols</h3>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$J$</td>
      <td>cost function</td>
    </tr>
    <tr>
      <td>$y_k$</td>
      <td>activation of a neon</td>
    </tr>
    <tr>
      <td>$u_k$</td>
      <td>input of a neon</td>
    </tr>
    <tr>
      <td>$S_i(\mathbf{u})$</td>
      <td>softmax function, $i$th value for a vector $\mathbf{u}$</td>
    </tr>
  </tbody>
</table>

<h3 id="formulas">Formulas</h3>

<p>Softmax of a vector $\mathbf{u}$ is defined as,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:softmax}
S_i(\mathbf{u}) = \frac{e^{u_i}}{\sum_k{e^{u_k}}} = y_i
\end{equation}</script>

<p>the derivative of softmax is,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} \label{eq:softmax_dev}
    \frac{\partial S_i(\mathbf{u})}{\partial u_j} =
    \frac{\partial y_i}{\partial u_j} = 
          \left\{\begin{array}{ll}
                        y_i(1-y_i) & i = j \\
                        -y_iy_j & i \neq j 
                \end{array} \right.
\end{equation} %]]></script>

<h3 id="layers-in-rnnlib">Layers in RNNLIB</h3>

<p>Every layer in RNNLIB consists of input and output sides,
both sides contain activations and errors.
Their relations with terms in math are shown in following table,</p>

<table>
  <thead>
    <tr>
      <th>Variable</th>
      <th style="text-align: center">Term</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>inputActivations</em></td>
      <td style="text-align: center">$u_k$</td>
    </tr>
    <tr>
      <td><em>outputActivations</em></td>
      <td style="text-align: center">$y_k$</td>
    </tr>
    <tr>
      <td><em>inputErrors</em></td>
      <td style="text-align: center">$\frac{\partial J}{\partial u_k}$</td>
    </tr>
    <tr>
      <td><em>outputErrors</em></td>
      <td style="text-align: center">$\frac{\partial J}{\partial y_k}$</td>
    </tr>
  </tbody>
</table>

<h2 id="forward-pass">Forward Pass</h2>

<p>Forward pass computes $y_k$ from $u_k$ using equation 
\eqref{eq:softmax}. There is a trick in the code, 
we can call it the <em>safe</em> softmax.</p>

<p>To understand it, consider dividing both numerator and denominator
by $e^c$ in equation \eqref{eq:softmax},</p>

<script type="math/tex; mode=display">\begin{equation}
S_i(\mathbf{u}) 
= \frac{\frac{e^{u_i}}{e^{c}}}{\frac{\sum_k{e^{u_k}}}{e^c}} 
= \frac{e^{u_i - c}}{\sum_k{e^{u_k - c}}} 
= S_i(\hat{\mathbf{u}})  
\end{equation}</script>

<p>thus, in order to avoid overflow when calculating exponentials<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, 
we can replace $u_k$ with $\hat{u}_k=u_k-c$. Typically, $c$ is set to $u_{max}$.</p>

<p>In RNNLIB, <script type="math/tex">c=\frac{u\_{max}+u\_{min}}{2}</script>.</p>

<h2 id="backpropagating">Backpropagating</h2>

<p>Backpropagation computes $\frac{\partial J}{\partial u_k}$ 
from $\frac{\partial J}{\partial y_k}$.</p>

<p>In RNNLIB, the result is</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:error_u_res}
\frac{\partial J}{\partial u_j} = y_j (\frac{\partial J}{\partial y_j} 
- \langle \mathbf{y}, \frac{\partial J}{\partial \mathbf{y}} \rangle)
\end{equation}</script>

<p>where, $\langle \cdot \, , \cdot \rangle$ denotes inner product.</p>

<p>To get the above equation, we first notice that variations in 
$u_j$ give rise to variations in the error function $J$ 
through variations in all $y_k$s. 
Thus, according to the <a href="https://www.math.hmc.edu/calculus/tutorials/multichainrule/">Multivariable Chain Rules</a>,
we can write,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:error_u}
\frac{\partial J}{\partial u_j} = \sum_k{\frac{\partial J}{\partial y_k}\frac{\partial y_k}{\partial u_j}}
\end{equation}</script>

<p>Using equation \eqref{eq:softmax_dev} to replace $\frac{\partial y_k}{\partial u_j}$, we get,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} 
\begin{split}
\frac{\partial J}{\partial u_j} &= y_j(1-y_j)\frac{\partial J}{\partial y_j} +
\sum_{k: k\neq j}{-y_k y_j \frac{\partial J}{\partial y_k}} \\
&= y_j(\frac{\partial J}{\partial y_j} -y_j \frac{\partial J}{\partial y_j} 
+ \sum_{k: k\neq j}{-y_k \frac{\partial J}{\partial y_k}}) \\
&= y_j(\frac{\partial J}{\partial y_j} - \sum_{k}{y_k \frac{\partial J}{\partial y_k}}) \\
&= y_j(\frac{\partial J}{\partial y_j} - \langle \mathbf{y}, \frac{\partial J}{\partial \mathbf{y}} \rangle)
\end{split}
\end{equation} %]]></script>

<p>Finally, we reach equation \eqref{eq:error_u_res}.</p>

<p>In this way, softmax operation can be implemented to be a standalone layer.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Strictly speaking, this converts overflow into underflow. 
  Underflow is no problem, because that rounds off to zero, which is a well-behaved floating point number.
  otherwise, it will be Infinity or NaN. see <a href="http://lingpipe-blog.com/2009/03/17/softmax-without-overflow/">this article</a> for details. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>

  <footer class='entry-footer' role='contentinfo'>
    
  </footer>

  <span class="entry-social-sharing social-links"><a class="twitter-share-link" href="https://twitter.com/intent/tweet?&text=RNNLIB%3A%20Softmax%20Layer%20%20%20-%20http%3A%2F%2Fwantee.github.io%2F2015%2F02%2F05%2Frnnlib-softmax-layer%2F" title="Share on Twitter">Twitter</a><a class="facebook-share-link" href="https://www.facebook.com/sharer/sharer.php?u=http://wantee.github.io/2015/02/05/rnnlib-softmax-layer/" title="Share on Facebook">Facebook</a><a class="g-plus-share-link" href="https://plus.google.com/share?url=http://wantee.github.io/2015/02/05/rnnlib-softmax-layer/" title="Share on Google+">Google+</a><a class="email-share-link" href="mailto:?subject=RNNLIB%3A%20Softmax%20Layer%20by%20Wantee%20Wang&body=RNNLIB%3A%20Softmax%20Layer%20by%20Wantee%20Wang%20-%20http%3A%2F%2Fwantee.github.io%2F2015%2F02%2F05%2Frnnlib-softmax-layer%2F" title="Share via email">Email</a></span>

</article>
<nav role="pagination" class="post-nav">

<a class="previous-post" href="/2015/02/05/rnnlib-introduction/" title="Previous Article: RNNLIB: Introduction">
  <div class='previous-post-marker'><span class='previous-post-arrow'>&larr;</span> Previous Article</div>
  <h6 class='previous-post-title'>RNNLIB: Introduction</h6>
</a>


<a class="next-post" href="/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/" title="Next Article: RNNLIB: Connectionist Temporal Classification and Transcription Layer">
  <div class='next-post-marker'>Next Article <span class='next-post-arrow'>&rarr;</span></div>
  <h6 class='next-post-title'>RNNLIB: Connectionist Temporal Classification and Transcription Layer</h6>
</a>

</nav>
<div class='entry-comments'><div id="disqus_thread"></div> <script type="text/javascript"> var disqus_shortname = 'wantee'; var disqus_url = 'http://wantee.github.io/2015/02/05/rnnlib-softmax-layer/'; var disqus_identifier = 'http://wantee.github.io/2015/02/05/rnnlib-softmax-layer/'; var disqus_title = 'RNNLIB: Softmax Layer'; (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//wantee.disqus.com/embed.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); }()); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript></div>
</div>
</div>
</div>
<div class="site-bottom"><div class="site-bottom-content">
<footer class='site-footer' role="contentinfo">
  <p class='footer-copyright'>Copyright © 2015- Wantee Wang
  - Powered by <a href='http://octopress.org'>Octopress</a></p>
  
</footer>
</div></div></div>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</div>
  
  
  
</body>
</html>
