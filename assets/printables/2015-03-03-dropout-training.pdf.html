
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Dropout Training - Formula Coding</title>
  <meta name="author" content="Wantee Wang">

  
  <meta name="description" content="\tableofcontents Dropout is a regularisation technique for reducing over-fitting in large neural nets. Hinton proposes the method in this paper. Most &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://wantee.github.io/assets/printables/2015-03-03-dropout-training.pdf.html">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Formula Coding" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-58316654-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Formula Coding</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="wantee.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Dropout Training</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-03-03T15:18:26+08:00'><span class='date'><span class='date-month'>Mar</span> <span class='date-day'>3</span><span class='date-suffix'>rd</span>, <span class='date-year'>2015</span></span> <span class='time'>3:18 pm</span></time>
        <span class="printable">


  


<a href="/assets/printables/.pdf"> <img src="/images/printer.png" alt="printable version"> </a>

</span>

        
      </p>
    
  </header>


<div class="entry-content"><p>\tableofcontents</p>

<p>Dropout is a regularisation technique for reducing over-fitting in large neural nets. Hinton proposes the method in <a href="http://arxiv.org/abs/1207.0580">this paper</a>. 
Most materials are from <a href="http://www.cs.toronto.edu/~nitish/dropout/">Srivastava’s page</a>.</p>

<p>It prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently. The term <em>dropout</em> refers to dropping out units (hidden and visible) in a neural network.</p>

<h1 id="the-method">The Method</h1>

<p>There are 2 key points for dropout learning: </p>

<ul>
  <li>a) Dropping units while training; </li>
  <li>b) Scaling output to be matched between training and testing. </li>
</ul>

<p>As shown in following figure (\autoref{fig:dropout}), where $p$ is the dropout retention.</p>

<p>\begin{figure}[h]\centering\includegraphics[width=\textwidth]{source//images/posts/Dropout.png}\caption{Dropout}\label{fig:dropout}\end{figure}</p>

<p>Units to be dropped is chosen in a random way. Note that dropping a unit out means temporarily removing it from the network, along with all its incoming and outgoing connections. Therefore we have to deal with it both during forward pass and backpropagation.</p>

<p>Applying dropout to a neural network amounts to sampling a <em>thinned</em> network from it. A neural net with $n$ units, can be seen as a collection of $2^n$ possible thinned neural networks. For each presentation of each training case, a new thinned network is sampled and trained. </p>

<p>At test time, the ideal way is to explicitly average the predictions from exponentially many thinned models, which is obviously not feasible. The intuitive way is using a single neural net without dropout at test time, however this needs some approximation. </p>

<p>The goal is that for any hidden unit the expected output (under the distribution used to drop units at training time) is the same as the actual output at test time.</p>

<p>Let $\mathbb{M}$ be the set of all thinned networks, and $\mathcal{M}$ be the network without dropout used in test time, i.e. the network containing all units. Note that, weights for all networks in $\mathbb{M}$ are shared and are equal to the ones in $\mathcal{M}$. Thus the expected output of a unit $j$ is </p>

<script type="math/tex; mode=display">
\mathbb{E}[\mathbf{y}_j] = \sum_{M \in \mathbb{M}}{Pr(M)\mathbf{y}^{M}_j}
</script>

<p>Where, $\mathbf{y}^{M}$ is the output of thinned network $M$.</p>

<table>
  <tbody>
    <tr>
      <td>Let $\mathbb{M}^{*}$ be the set of networks in which unit $j$ is active, then $</td>
      <td>\mathbb{M}^{*}</td>
      <td>= p</td>
      <td>\mathbb{M}</td>
      <td>$. If we assume that the probability of $M$s are equal, i.e. $Pr(M) = \frac{1}{</td>
      <td>\mathbb{M}</td>
      <td>}$, and assume $\mathbf{y}^{M} = \mathbf{y}^{\mathcal{M}}$, we get,</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">
\mathbb{E}[\mathbf{y}_j] = p|\mathbb{M}| \frac{1}{|\mathbb{M}|} \mathbf{y}^{\mathcal{M}}_j = p\mathbf{y}^{\mathcal{M}}_j
</script>

<p>At this point, there are two method to match the training output and testing output.
First one, by scaling down the weight used at test time, i.e. $\mathbf{w}’<em>{ji} = p\mathbf{w}</em>{ji}$, we can achieve the goal. This is the way used in the above paper and shown in the figure.</p>

<p>The second way is to scale up the output at training time to the same magnitude as test time, i.e. $\mathbf{y}’_{j} = \frac{1}{p}\mathbf{y}_j$.</p>

<h1 id="implementation-in-kaldi">Implementation in Kaldi</h1>

<p>Both Karel’s and Dan’s implementation have the Dropout codes, with some differences.</p>

<p>Karel’s code(<code>src/nnet/nnet-activation.h:Dorpout</code>) uses the scale-up method to get the expected output. Dropping out is implemented during forward pass and by storing the dropped out units using a 0/1 vector, the back-propagated derivative can be set properly.</p>

<p>Dan’s code(<code>src/nnet2/net-component.cc:DropoutComponent</code>) use a clever way to avoid storing the dropping units. While backpropagation, we can get the input error $\mathbf{e_i}$ from output error $\mathbf{e_i}$ by</p>

<script type="math/tex; mode=display">
\mathbf{e_i} = \frac{\mathbf{a_o}}{\mathbf{a_i}} \mathbf{e_o}
</script>

<p>where $\mathbf{a_i}$ and $\mathbf{a_o}$ is the activation of input and output for Dropout component. Elements in $\mathbf{a_o}$ is the equal to the corresponding scaled value in $\mathbf{a_i}$, which maybe zero if it is the dropping ones.</p>

<p>Dan’s code applies a more general form of scaling. Instead of set the output of dropping unit to zero, we can just scale the output value by a factor $\alpha$. To get a proper scaled version of output, we’d like to scale all the units besides the dropping ones and make it satisfy that the expected scale factor should be 1, i.e.,</p>

<script type="math/tex; mode=display">
q \alpha + (1-q)\beta = 1
</script>

<p>where, $q=1-p$ is the dropout proportion. Therefore, we can get the factor of other units $\beta = \frac{1-q\alpha}{1-q}$. If we set $\alpha=0$, then $\beta=\frac{1}{1-q}=\frac{1}{p}$, which is equal to the scale-up factor.</p>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Wantee Wang</span></span>

      




<time class='entry-date' datetime='2015-03-03T15:18:26+08:00'><span class='date'><span class='date-month'>Mar</span> <span class='date-day'>3</span><span class='date-suffix'>rd</span>, <span class='date-year'>2015</span></span> <span class='time'>3:18 pm</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/neural-network/'>Neural Network</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://wantee.github.io/assets/printables/2015-03-03-dropout-training.pdf.html" data-via="" data-counturl="http://wantee.github.io/assets/printables/2015-03-03-dropout-training.pdf.html" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/03/03/dropout-training/">Dropout Training</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/">RNNLIB: Connectionist Temporal Classification and Transcription Layer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/05/rnnlib-softmax-layer/">RNNLIB: Softmax Layer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/05/rnnlib-introduction/">RNNLIB: Introduction</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/wantee">@wantee</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'wantee',
            count: 0,
            skip_forks: true,
            skip_repos: [ "wantee.github.io" ],
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Wantee Wang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'wantee';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://wantee.github.io/assets/printables/2015-03-03-dropout-training.pdf.html';
        var disqus_url = 'http://wantee.github.io/assets/printables/2015-03-03-dropout-training.pdf.html';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>






  <!-- mathjax config similar to math.stackexchange -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$']],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</body>
</html>
