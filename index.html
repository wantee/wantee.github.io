
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Formula Coding</title>
  <meta name="author" content="Wantee Wang">

  
  <meta name="description" content="The Method Implementation in Kaldi Dropout is a regularisation technique for reducing over-fitting in large neural nets. Hinton proposes the method &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://wantee.github.io/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Formula Coding" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-58316654-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Formula Coding</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="wantee.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/03/03/dropout-training/">Dropout Training</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-03-03T15:18:26+08:00'><span class='date'><span class='date-month'>Mar</span> <span class='date-day'>3</span><span class='date-suffix'>rd</span>, <span class='date-year'>2015</span></span> <span class='time'>3:18 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><ul id="markdown-toc">
  <li><a href="#the-method">The Method</a></li>
  <li><a href="#implementation-in-kaldi">Implementation in Kaldi</a></li>
</ul>

<p>Dropout is a regularisation technique for reducing over-fitting in large neural nets. Hinton proposes the method in <a href="http://arxiv.org/abs/1207.0580">this paper</a>
Most materials is from <a href="http://www.cs.toronto.edu/~nitish/dropout/">Srivastava’s page</a>.</p>

<p>It prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently. The term “dropout” refers to dropping out units (hidden and visible) in a neural network.</p>

<h2 id="the-method">The Method</h2>

<p>There are 2 key points for dropout learning: </p>

<ul>
  <li>a) Dropping units while training; </li>
  <li>b) Scaling weight while testing. </li>
</ul>

<p>As shown in following figure, where $p$ is the dropout retention rate.</p>

<p><img class="center" src="/images/posts/Dropout.png" title="Dropout" /></p>

<p>Units to be dropped is chosen in a random way. Note that dropping a unit out means temporarily removing it from the network, along with all its incoming and outgoing connections. Therefore we have to deal with it both during forward pass and backpropagation.</p>

<p>Applying dropout to a neural network amounts to sampling a “thinned” network from it. A neural net with $n$ units, can be seen as a collection of $2^n$ possible thinned neural networks. For each presentation of each training case, a new thinned network is sampled and trained. </p>

<p>At test time, the ideal way is to explicitly average the predictions from exponentially many thinned models, which is obviously not feasible. However the network with scaled weights gives a good approximation. </p>

<p>The goal is that for any hidden unit the expected output (under the distribution used to drop units at training time) is the same as the actual output at test time.</p>

<p>The expected output of a unit is </p>

<script type="math/tex; mode=display">
\mathbb{E}[\mathbf{y}_j] = \sum_i{p \mathbf{w}_{ji} \mathbf{x}_i}
</script>

<p>Therefore, by scaling down the weight used at test time, i.e. $\mathbf{w}’_{ji} = p\mathbf{w}_{ji}$, we can achieve the goal. This is the way used in the above paper and shown in the figure.</p>

<p>Another way is to scale up the output at training time to the same magnitude as test time, i.e. $\mathbf{y}’_{j} = \frac{1}{p}\mathbf{y}_j$.</p>

<h2 id="implementation-in-kaldi">Implementation in Kaldi</h2>

<p>Both Karel’s and Dan’s implementation have the Dropout codes, with some differences.</p>

<p>Karel’s code(<code>src/nnet/nnet-activation.h:Dorpout</code>) uses the scale-up method to get the expected output. Dropping out is implemented during forward pass and by storing the dropped out units using a 0/1 vector, the back-propagated derivative can be set properly.</p>

<p>Dan’s code(<code>src/nnet2/net-component.cc:DropoutComponent</code>) use a clever way to avoid storing the dropping units. While backpropagation, we can get the input error $\mathbf{e_i}$ from output error $\mathbf{e_i}$ by</p>

<script type="math/tex; mode=display">
\mathbf{e_i} = \frac{\mathbf{a_o}}{\mathbf{a_i}} \mathbf{e_o}
</script>

<p>where $\mathbf{a_i}$ and $\mathbf{a_o}$ is the activation of input and output for Dropout component. Elements in $\mathbf{a_o}$ is the equal to that in $\mathbf{a_i}$ except the dropping ones, which is zero.</p>

<p>Dan’s code seems to not care about the scaling problem, we can scale the final network before testing using the scale-down method. However, there is another form of scale in Dan’s code, instead of set the output of dropping unit to zero, we just scale the output value by a factor $\alpha$. To get a proper scaled version of output, we’d like to scale all the units besides the dropping ones and make it satisfy that the expected scale factor should be 1, i.e.,</p>

<script type="math/tex; mode=display">
p \alpha + (1-p)\beta = 1
</script>

<p>Therefore, we can get the factor of other units $\beta = \frac{1-p\alpha}{1-p}$.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/">RNNLIB: Connectionist Temporal Classification and Transcription Layer</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-02-08T16:56:40+08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>8</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>4:56 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><ul id="markdown-toc">
  <li><a href="#the-name">The Name</a></li>
  <li><a href="#the-theory">The Theory</a>    <ul>
      <li><a href="#list-of-symbols">List of Symbols</a></li>
      <li><a href="#training-procedure">Training Procedure</a></li>
      <li><a href="#the-ctc-forward-backward-algorithm">The CTC Forward-Backward Algorithm</a></li>
    </ul>
  </li>
  <li><a href="#the-implementation">The Implementation</a></li>
  <li><a href="#decoding">Decoding</a>    <ul>
      <li><a href="#best-path-decoding">Best Path Decoding</a></li>
      <li><a href="#prefix-search-decoding">Prefix Search Decoding</a></li>
    </ul>
  </li>
</ul>

<p>CTC is the core concept make it possible to transcribe unsegmented sequence data.
RNNLIB implements it in a single layer called Transcription Layer.
We go into this particular layer in this post, the main reference is the Graves’
<a href="http://www6.in.tum.de/pub/Main/Publications/Graves2006a.pdf">original paper</a>.</p>

<p>The key point for CTC is to use a simple map transforming the RNN output to unsegmented labelling,
and construct a new objective function based on the map.
This map do not need a precise alignment, thus greatly simplify the task and reduce human expert involvement. </p>

<h2 id="the-name">The Name</h2>

<p>“Connectionist” is the adjective form of “connectionism”, 
<a href="http://en.wikipedia.org/wiki/Connectionism">Connectionism</a> is a terminology in cognitive science,
which models mental or behavioural phenomena as the emergent processes of interconnected networks of simple units.
The most common forms use neural network models. </p>

<p>In the traditional neural network recipe, we independently model the input sequence 
in each time-step or frame. This can be referred as <em>framewise classification</em>.
<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.8007&amp;rep=rep1&amp;type=pdf">Kadous</a> 
extends the classification paradigm to multivariate time series, and 
names it as <em>temporal classification</em>. Mathematically, 
framewise classification models the distribution over output sequences of the <em>same</em> length as the input sequence,
nevertheless,  temporal classification models the distribution over output sequences of <em>all</em> lengths.
With this, we do not have to label every time step in training data set.</p>

<p>Combining RNN and temporal classification, Graves proposes the <em>connectionist temporal classification</em>.</p>

<p>To distinguish from classification, RNNLIB implements the CTC as <em>Transcription Layer</em>, 
indicating that with CTC we can directly transcribe input sequence(e.g. acoustic signal)
into output sequence(e.g. words).</p>

<h2 id="the-theory">The Theory</h2>

<h3 id="list-of-symbols">List of Symbols</h3>

<p>Following the notations in the paper, we first list the symbols.</p>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$L$</td>
      <td>(finite) alphabet of labels</td>
    </tr>
    <tr>
      <td>$L’$</td>
      <td>$L \cup \{blank\}$</td>
    </tr>
    <tr>
      <td>$\mathcal{X}$</td>
      <td>$(\mathbb{R}^m)^{*}$, $m$ dimensional input space</td>
    </tr>
    <tr>
      <td>$\mathcal{Z}$</td>
      <td>$L^{*}$, output space, set of all sequences over the $L$</td>
    </tr>
    <tr>
      <td>$\mathcal{D_{X \times Z}}$</td>
      <td>underlying distribution of data</td>
    </tr>
    <tr>
      <td>$S$</td>
      <td>set of training examples supposed to be drawn from $\mathcal{D_{X \times Z}}$</td>
    </tr>
    <tr>
      <td>($\mathbf{x},\mathbf{z})$</td>
      <td>example in $S$, $\mathbf{x} = (x_1, x_2, \dotsc, x_T)$, $\mathbf{z} = (z_1, z_2, \dotsc, z_U)$ and $U \leq T$</td>
    </tr>
    <tr>
      <td>$h:\mathcal{X} \mapsto \mathcal{Z}$</td>
      <td>temporal classifier to be trained</td>
    </tr>
    <tr>
      <td>$\mathcal{N}_{w}:(R^{m})^{T} \mapsto (R^n)^{T}$</td>
      <td>RNN, with $m$ inputs, $n$ outputs and weight vector $w$, as a continuous map</td>
    </tr>
    <tr>
      <td>$\mathbf{y} = \mathcal{N}_{w}$</td>
      <td>sequence of RNN output</td>
    </tr>
    <tr>
      <td>$y_{k}^{t}$</td>
      <td>the activation of output unit $k$ at time $t$</td>
    </tr>
    <tr>
      <td>$\pi$</td>
      <td><em>path</em>, element of $L’^{T}$</td>
    </tr>
    <tr>
      <td>$\mathbf{l} \in L^{\leq T}$</td>
      <td>label sequence or <em>labelling</em></td>
    </tr>
    <tr>
      <td>$\mathcal{B}:L’^{T} \mapsto L^{\leq T}$</td>
      <td>map from path to labelling</td>
    </tr>
    <tr>
      <td>$\mathbf{l}_{a\mathord{:}b}$</td>
      <td>sub-sequence of $\mathbf{l}$ from $a$th to $b$th labels</td>
    </tr>
    <tr>
      <td>$\mathbf{l}’$</td>
      <td>modified label sequence, with blanks added to the beginning and the end and inserted between every pair of labels in $\mathbf{l}$</td>
    </tr>
    <tr>
      <td>$\alpha_t(s)$</td>
      <td>forward variable, the total probability of $\mathbf{l}_{1:s}$ at time $t$</td>
    </tr>
    <tr>
      <td>$\beta_t(s) $</td>
      <td>backward variable, the total probability of $\mathbf{l}_{s:|\mathbf{l}’|}$ at time $t$</td>
    </tr>
    <tr>
      <td>$\tilde{\beta}_t(s) $</td>
      <td>backward variable, the total probability of $\mathbf{l}_{s:|\mathbf{l}’|}$ start at time $t+1$</td>
    </tr>
    <tr>
      <td>$O^{ML}(S,\mathcal{N}_{w})$</td>
      <td>maximum likelihood objective function</td>
    </tr>
    <tr>
      <td>$\delta_{kk’}$</td>
      <td><a href="http://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a></td>
    </tr>
  </tbody>
</table>

<h3 id="training-procedure">Training Procedure</h3>

<p>The goal is to use $S$ to train a temporal classifier $h$ to classify previously unseen input sequences in a way that minimises the ML objective function:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:obj_ml}
O^{ML}(S,\mathcal{N}_{w}) = - \sum_{(\mathbf{x},\mathbf{z})\in S}{\ln(p(\mathbf{z}|\mathbf{x}))}
\end{equation}</script>

<p>To train the network with gradient descent, 
we need to differentiate \eqref{eq:obj_ml} with respect to the network outputs. 
Since the training examples are independent we can consider them separately:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:obj}
\frac{\partial O^{ML}(\{(\mathbf{x},\mathbf{z}\},\mathcal{N}_{w})}{\partial y_k^t} 
    = - \frac{\partial \ln(p(\mathbf{z}|\mathbf{x}))}{\partial y_k^t}
    = - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{\partial p(\mathbf{z}|\mathbf{x})}{\partial y^t_k}
\end{equation}</script>

<p>Another thing we have to consider is how to map from network outputs to labellings.
Use $\mathcal{B}$ to denote such a map. Given a path, we simply removing all blanks 
and repeated labels and the remaining labels form a labelling(e.g. $\mathcal{B}(a-ab-)=\mathcal{B}(-aa–abb)=aab$). </p>

<p>Then we can define the conditional probability of a labelling,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:labelling}
p(\mathbf{l}|\mathbf{x}) = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{l})}{p(\pi|\mathbf{x})}
\end{equation}</script>

<p>where, $p(\pi|\mathbf{x})$ is the conditional probability of a path given $\mathbf{x}$, and is defined as:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:path}
p(\pi|\mathbf{x}) = \prod_{t=1}^{T}{y_{\pi_t}^{t}},\forall \pi \in L'^{T}
\end{equation}</script>

<p>To calculate \eqref{eq:obj}, we first define the forward and backward variable,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:fwd}
\alpha_t(s) = \sum_{\pi \in L^{T}:\mathcal{B}(\pi_{1\mathord{:}t})=\mathbf{l}_{1\mathord{:}s}}
   {\prod_{t'=1}^{t}{y^{t'}_{\pi_{t'}}}}
\end{equation}</script>

<script type="math/tex; mode=display">\begin{equation} \label{eq:bwd}
\beta_t(s) = \sum_{\pi \in L^{T}:\mathcal{B}(\pi_{t\mathord{:}T})=\mathbf{l}_{s\mathord{:} |\mathbf{l}'|}}
   {\prod_{t'=t}^{T}{y^{t'}_{\pi_{t'}}}}
\end{equation}</script>

<p>Note that the product of the forward and backward variables at a given $s$ and $t$ is the probability of all the paths corresponding to $\mathbf{l}$ that go through the symbol $s$ at time $t$, i.e.,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:fwd_bwd_ori}
\alpha_t(s)\beta_t(s) = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{l}):\pi_{t}=\mathbf{l}'_{s}}
   {y^{t}_{\mathbf{l}'_{s}}\prod_{t=1}^{T}{y^{t}_{\pi_{t}}}}
\end{equation}</script>

<p>Rearranging and substituting in from \eqref{eq:path} gives,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:fwd_bwd}
\frac{\alpha_t(s)\beta_t(s)}{y^{t}_{\mathbf{l}'_{s}}} = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{l}):\pi_{t}=\mathbf{l}'_{s}}
   {p(\pi|\mathbf{x})}
\end{equation}</script>

<p>For any $t$, we can therefore sum over all $s$ to get $p(\mathbf{l} | \mathbf{x})$:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:labelling_fwd_bwd}
p(\mathbf{l}|\mathbf{x}) = \sum_{s=1}^{|\mathbf{l}'|}\frac{\alpha_t(s)\beta_t(s)}{y^{t}_{\mathbf{l}'_{s}}}
\end{equation}</script>

<p>On the other hand, combining \eqref{eq:labelling} and \eqref{eq:path},</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:labelling_all}
p(\mathbf{l}|\mathbf{x}) = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{l})}{\prod_{t=1}^{T}{y_{\pi_t}^{t}}}
\end{equation}</script>

<p>Thus to differentiate this with respect to $y_k^t$ , 
we need only consider those paths going through label $k$ at time $t$
(derivatives of other paths is zero). 
Noting that the same label (or blank) may be repeated several times for a single labelling $\mathbf{l}$, 
we define the set of positions where label $k$ occurs as $lab(\mathbf{l},k) = \{s : \mathbf{l}’_s = k\}$, 
which may be empty. We then differentiate \eqref{eq:labelling_all} to get,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} \label{eq:labelling_drv}
\begin{split} 
\frac{\partial p(\mathbf{l}|\mathbf{x})}{\partial y_k^t} 
  &= \sum_{s \in lab(\mathbf{l}, k)}{\frac{\partial p(\pi|\mathbf{x})}{\partial y_k^t}} \\
  &= \sum_{s \in lab(\mathbf{l}, k)}{\frac{\partial \prod_{t'=1}^{T}{y_{\pi_{t'}}^{t'}}}{\partial y_k^t}} \\
  &= \sum_{s \in lab(\mathbf{l}, k)}{\frac{\partial y_k^t\prod_{t' \neq t}{y_{\pi_{t'}}^{t'}}}{\partial y_k^t}} \\
  &= \sum_{s \in lab(\mathbf{l}, k)}{\prod_{t' \neq t}{y_{\pi_{t'}}^{t'}}} \\
  &= \frac{1}{ {y^t_k}^2 } \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\beta_t(s)}
\end{split} 
\end{equation} %]]></script>

<p>At this point, we can set $\mathbf{l} = \mathbf{z}$ and substituting into \eqref{eq:obj}, then get the final gradient,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:grad}
\frac{\partial O^{ML}(\{(\mathbf{x},\mathbf{z}\},\mathcal{N}_{w})}{\partial y_k^t} 
    = - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ {y^t_k}^2 } \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\beta_t(s)}
\end{equation}</script>

<p>where, $p(\mathbf{z}|\mathbf{x})$ can be calculated from \eqref{eq:labelling_fwd_bwd}.</p>

<p>Next, we can give the gradient for the unnormalised output $u_k^t$. Recall that derivative of softmax function is, </p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:err_softmax}
\frac{\partial y^t_{k'}}{\partial u_k^t} = y^t_{k'}\delta_{kk'} - y^t_{k'}y^t_k
\end{equation}</script>

<p>Then we get,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} \label{eq:error_u}
\begin{split} 
\frac{\partial O}{\partial u_k^t} 
  &= \sum_{k'}{\frac{\partial O}{\partial y^t_{k'}}\frac{\partial y^t_{k'}}{\partial u^t_k}} \\
  &= \sum_{k'}{((y^t_{k'}\delta_{kk'} - y^t_{k'}y^t_k) (-\frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ {y^t_{k'}}^2 } 
                \sum_{s \in lab(\mathbf{l}, k')}{\alpha_t(s)\beta_t(s)}))} \\
  &= \sum_{k'}{(\frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{y^t_{k}}{ {y^t_{k'}} } 
                \sum_{s \in lab(\mathbf{l}, k')}{\alpha_t(s)\beta_t(s)})} - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ {y^t_k} } 
                \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\beta_t(s)}\\  
  &= y^t_k - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ {y^t_k} } 
                \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\beta_t(s)}\\                               
\end{split} 
\end{equation} %]]></script>

<p>we write the last step by noting that $\sum_{k’}\sum_{s \in lab(\mathbf{l}, k’)}{(\cdot)} \equiv \sum_{s=1}^{|\mathbf{l}’|}{(\cdot)}$,
then, using \eqref{eq:labelling_fwd_bwd}, the $p(\mathbf{z}|\mathbf{x})$ is canceled out.</p>

<h3 id="the-ctc-forward-backward-algorithm">The CTC Forward-Backward Algorithm</h3>

<p>The last thing we have to do is calculating the forward and backward variables. We now show that by define a recursive from, 
these variables can be calculated efficiently.</p>

<p>Given a labelling $\mathbf{l}$, we first extend it to $\mathbf{l}’$ with blanks added to the beginning 
and the end and inserted between every pair of labels. 
The length of $\mathbf{l}’$ is therefore $2|\mathbf{l}| + 1$. 
In calculating the probabilities of prefixes of $\mathbf{l}’$ we allow all transitions between blank and non-blank labels, 
and also those between any pair of distinct non-blank labels(because of the map $\mathcal{B}$, the repeated labels will be merged). 
We allow all prefixes to start with either a blank ($b$) or the first symbol in $\mathbf{l}$ ($\mathbf{l}_1$).</p>

<p>This gives us the following rules for initialisation</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{split} 
\alpha_1(1) &= y_b^1 \\ 
\alpha_1(2) &= y_{\mathbf{l}_1}^1 \\ 
\alpha_1(s) &= 0, \forall s > 2 \\                               
\end{split} 
 %]]></script>

<p>and recursion</p>

<script type="math/tex; mode=display">% <![CDATA[
 \begin{equation} \label{eq:alpha}
    \alpha_t(s) = \left\{\begin{array}{ll}
                                    y_{\mathbf{l}'_s}^t(\alpha_{t-1}(s) + \alpha_{t-1}(s-1)) & \mathbf{l}'_s = b\, \text{or} \, \mathbf{l}'_{s-2} = \mathbf{l}'_s\\
                                    y_{\mathbf{l}'_s}^t(\alpha_{t-1}(s) + \alpha_{t-1}(s-1) + \alpha_{t-1}(s-2)) & otherwise
                                \end{array} \right.
\end{equation} %]]></script>

<p>Note that $\alpha_t(s) = 0, \forall s &lt; |\mathbf{l}’|−2(T −t)−1$, 
because these variables correspond to states for which there are not enough time-steps left to complete the sequence.</p>

<p>Here we can get another method to calculate $p(\mathbf{l} | \mathbf{x})$, by adding up all forward variables at time $T$, i.e.,</p>

<script type="math/tex; mode=display"> \begin{equation} \label{eq:porb}
    p(\mathbf{l} | \mathbf{x}) = \alpha_T(|\mathbf{l}'|) + \alpha_T(|\mathbf{l}'| - 1)
\end{equation}</script>

<p>Similarly, the backward variables can be initalisd as,</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{split} 
\beta_T(|\mathbf{l}'|) &= y_b^T \\ 
\beta_T(|\mathbf{l}'| - 1) &= y_{\mathbf{l}_{|\mathbf{l}|}}^T \\ 
\beta_T(s) &= 0, \forall s< |\mathbf{l}'| - 1\\                               
\end{split} 
 %]]></script>

<p>and recursion</p>

<script type="math/tex; mode=display">% <![CDATA[
 \begin{equation} \label{eq:beta}
    \beta_t(s) = \left\{\begin{array}{ll}
                y_{\mathbf{l}'_s}^t(\beta_{t+1}(s) + \beta_{t+1}(s+1)) & \mathbf{l}'_s = b\, \text{or} \, \mathbf{l}'_{s+2} = \mathbf{l}'_s\\
                y_{\mathbf{l}'_s}^t(\beta_{t+1}(s) + \beta_{t+1}(s+1) + \beta_{t+1}(s+2)) & otherwise
                  \end{array} \right.
\end{equation} %]]></script>

<p>Note that $\beta_t(s) = 0, \forall s &gt; 2t$.</p>

<p>Following figure illustrate the forward backward algorithm applied to the labelling ‘CAT’(from the paper).</p>

<p><img class="center" src="/images/posts/CTC-alpha-beta.png" title="Alpha-Beta Algorithm" /></p>

<h2 id="the-implementation">The Implementation</h2>

<p>The <code>TranscriptionLayer</code> class inherits the <code>SoftmaxLayer</code> class(see <a href="/blog/2015/02/05/rnnlib-softmax-layer/">this post</a>).
The <code>feed_forward()</code> and <code>feed_back()</code> methods are the general softmax function, 
so only need to implement the <code>calculate_errors()</code> method to calculate the $\frac{\partial O}{\partial y_k^t}$.
In order to use \eqref{eq:grad} to get output error, first need to calculate the $\alpha$s and $\beta$s.
Forward variables are got using \eqref{eq:alpha}. </p>

<p>But backward variables are in another form, given in Graves’ <a href="www6.in.tum.de/Main/Publications/Graves2008c.pdf">Dissertation</a>.
Consider backward variable started from time $t+1$,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:bwd_new}
\tilde\beta_t(s) = \sum_{\pi \in L^{T}:\mathcal{B}(\pi_{t\mathord{:}T})=\mathbf{l}_{s\mathord{:} |\mathbf{l}'|}}
   {\prod_{t'=t+1}^{T}{y^{t'}_{\pi_{t'}}}}
\end{equation}</script>

<p>Noting that, $\beta$ and $\tilde\beta$ has a simple relationship:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:bwd_relaion}
\beta_t(s) = y_{\pi_{t}}^t\tilde\beta_t(s)
\end{equation}</script>

<p>Thus, we can get recursion formula for $\tilde\beta$ by substituting \eqref{eq:bwd_relaion} into \eqref{eq:beta},</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{split} 
\tilde\beta_T(|\mathbf{l}'|) &= 1 \\ 
\tilde\beta_T(|\mathbf{l}'| - 1) &= 1 \\ 
\tilde\beta_T(s) &= 0, \forall s< |\mathbf{l}'| - 1\\                               
\end{split} 
 %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
 \begin{equation} \label{eq:beta_new}
    \tilde\beta_t(s) = \left\{\begin{array}{ll}
                y_{\mathbf{l}'_s}^{t+1}\tilde\beta_{t+1}(s) + y_{\mathbf{l}'_{s+1}}^{t+1}\tilde\beta_{t+1}(s+1) & \mathbf{l}'_s = b\, \text{or} \, \mathbf{l}'_{s+2} = \mathbf{l}'_s\\
                y_{\mathbf{l}'_s}^{t+1}\tilde\beta_{t+1}(s) + y_{\mathbf{l}'_{s+1}}^{t+1}\tilde\beta_{t+1}(s+1) + y_{\mathbf{l}'_{s+2}}^{t+1}\tilde\beta_{t+1}(s+2) & otherwise
                \end{array} \right.
\end{equation} %]]></script>

<p>Noting that, if $\mathbf{l}’_s \neq blank$, then $\mathbf{l}’_{s+1}$ must be $blank$.</p>

<p>And the gradient for output \eqref{eq:grad} becomes,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:grad_new}
\frac{\partial O^{ML}(\{(\mathbf{x},\mathbf{z}\},\mathcal{N}_{w})}{\partial y_k^t} 
    = - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ y^t_k } \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\tilde\beta_t(s)}
\end{equation}</script>

<p>where,</p>

<script type="math/tex; mode=display">\begin{equation}
p(\mathbf{z}|\mathbf{x}) = \sum_{s=1}^{|\mathbf{z}'|}{\alpha_t(s)\tilde\beta_t(s)}
\end{equation}</script>

<p>Actually, the RNNLIB code computes $p(\mathbf{z}|\mathbf{x})$ using \eqref{eq:porb}.</p>

<p>To wrap up, CTC using a forward-backward algorithm to efficiently compute the RNN output errors, 
corresponding to a new ML objective function. With these errors, 
we can use any traditional gradient methods to train the network.</p>

<h2 id="decoding">Decoding</h2>

<p>Once the network is trained, we would use it to transcribe some unknown input sequence $\mathbf{x}$.
<em>Decoding</em> is referred to the task of finding the best labelling $\mathbf{l}^*$,</p>

<script type="math/tex; mode=display">\begin{equation}
\mathbf{l}^* = \mathop{\arg\!\max}\limits_{\mathbf{l}}{\,p(\mathbf{l}|\mathbf{x})}
\end{equation}</script>

<p>There are two approximate algorithms.</p>

<h3 id="best-path-decoding">Best Path Decoding</h3>

<p>This method assumes that the most probable path corresponding to the most probable labelling,</p>

<script type="math/tex; mode=display">\begin{equation}
\mathbf{l}^* \approx \mathcal{B}(\pi^*)
\end{equation}</script>

<p>where $\pi^* = \mathop{\arg\!\max}\limits_{\pi}{\,p(\pi|\mathbf{x})}$.</p>

<p>This is trivial to compute, simply by concatenating the most active outputs at every time step.
But it can lead to errors, because that the map $\mathcal{B}$ is a many-to-one map.</p>

<h3 id="prefix-search-decoding">Prefix Search Decoding</h3>

<p>By modifying the forward variables, this method can efficiently calculate the probabilities of successive extensions of labelling prefixes.</p>

<p>Prefix search decoding is a best-first search through the tree of labellings, 
where the children of a given labelling are those that share it as a prefix. 
At each step the search extends the labelling whose children have the largest cumulative probability (see below figure).</p>

<p><img class="center" src="/images/posts/CTC-prefix-decoding.png" title="Prefix Search Decoding" /> </p>

<p>Each node either ends ($e$) or extends the prefix at its parent node. 
The number above an extending node is the total probability of all labellings beginning with that prefix. 
The number above an end node is the probability of the single labelling ending at its parent. 
At every iteration the extensions of the most probable remaining prefix are explored. 
Search ends when a single labelling (here $XY$) is more probable than any remaining prefix.</p>

<p>To extend the tree, we need to compute extended path probability, which can be computed in a recursive way.
Let $\gamma_t(\mathbf{p}_n)$ be the probability of the network outputting prefix $\mathbf{p}$
by time $t$ such that a non-blank label is output at $t$. Similarly, let $\gamma_t(\mathbf{p}_b)$ be the
probability of the network outputting prefix $\mathbf{p}$ by time $t$ such that the blank label is output at $t$. i.e.</p>

<script type="math/tex; mode=display">\begin{equation}
\gamma_t(\mathbf{p}_n) = p(\pi_{1\mathord{:}t} : \mathcal{B}(\pi_{1\mathord{:}t}) = \mathbf{p}, \pi_t = \mathbf{p}_{ | \mathbf{p} | } \mid \mathbf{x})
\end{equation}</script>

<script type="math/tex; mode=display">\begin{equation}
\gamma_t(\mathbf{p}_b) = p(\pi_{1\mathord{:}t} : \mathcal{B}(\pi_{1\mathord{:}t}) = \mathbf{p}, \pi_t = blank \mid \mathbf{x})
\end{equation}</script>

<p>Then for a length $T$ input sequence $\mathbf{x}$, $p(\mathbf{p} | \mathbf{x}) = \gamma_T(\mathbf{p}_n) + \gamma_T(\mathbf{p}_b)$.
Also let $p(\mathbf{p}\dots | \mathbf{x})$ be the cumulative probability of all labelling not equal to $\mathbf{p}$ 
of which <script type="math/tex">\mathbf{p}</script> is a prefix</p>

<script type="math/tex; mode=display">\begin{equation}
p(\mathbf{p} \dotsc \mid \mathbf{x}) = \sum_{\mathbf{l} \neq \emptyset}{p(\mathbf{p} + \mathbf{l} \mid \mathbf{x})}
\end{equation}</script>

<p>where $\emptyset$ is the empty sequence. $p(\mathbf{p} \dotsc \mid \mathbf{x})$ is the value for extending node
in the prefix tree, and $p(\mathbf{p} \mid \mathbf{x})$ is the value for end node.</p>

<p>In fact, by definition, relation between $\gamma$ and $\alpha$ is,</p>

<script type="math/tex; mode=display"> \begin{equation}
    \gamma_t(\mathbf{p}_n) = \alpha_t(2 | \mathbf{p} |)
\end{equation}</script>

<script type="math/tex; mode=display"> \begin{equation}
    \gamma_t(\mathbf{p}_b) = \alpha_t(2 | \mathbf{p} | + 1)
\end{equation}</script>

<p>Using \eqref{eq:alpha}, we get the recursion for $\gamma_t(\mathbf{p}_n)$ given $\gamma_{t-1}(\mathbf{p}_n)$,
extending $\mathbf{p}^*$ to $\mathbf{p} = \mathbf{p}^* + k$ with label $k \in L$,</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{split} 
\gamma_1(\mathbf{p}_n) &= \left\{\begin{array}{ll}
            y_k^1 & \mathbf{p}^* = \emptyset \\
            0 & \text{otherwise}
                \end{array} \right. \\ 
\gamma_1(\mathbf{p}_b) &= 0 \\                              
\end{split} 
 %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
 \begin{equation}
    \gamma_t(\mathbf{p}_n) = \left\{\begin{array}{ll}
                y_k^t(\gamma_{t-1}(\mathbf{p}^*_b) + \gamma_{t-1}(\mathbf{p}_n)) & \mathbf{p}^* \,\,\text{ends in} \,\, k \\
                y_k^t(\gamma_{t-1}(\mathbf{p}^*_b) + \gamma_{t-1}(\mathbf{p}^*_n) + \gamma_{t-1}(\mathbf{p}_n))  & otherwise
                \end{array} \right.
\end{equation} %]]></script>

<script type="math/tex; mode=display"> \begin{equation}
    \gamma_t(\mathbf{p}_b) = y_b^t(\gamma_{t-1}(\mathbf{p}_b) + \gamma_{t-1}(\mathbf{p}_n))
\end{equation}</script>

<p>And calculating the path probabilities,</p>

<script type="math/tex; mode=display"> \begin{equation}
    p(\mathbf{p} \mid \mathbf{x}) = \gamma_{T}(\mathbf{p}_b) + \gamma_{T}(\mathbf{p}_n)
\end{equation}</script>

<script type="math/tex; mode=display"> \begin{equation}
    p(\mathbf{p}\dotsc \mid \mathbf{x}) = \gamma_1(\mathbf{p}_n) + \sum_{t=2}^{T}{(\gamma_{t}(\mathbf{p}_n) - y^t_k \gamma_{t-1}(\mathbf{p}_n))} - p(\mathbf{p} \mid \mathbf{x})
\end{equation}</script>

<p>The extension procedure start from $\mathbf{p}^* = \emptyset$, with initialisation,</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{split} 
1 \leq t \leq T & \left\{\begin{array}{ll}
            \gamma_t(\emptyset_n) &= 0 \\
            \gamma_t(\emptyset_b) &= \prod_{t'=1}^{t}y_b^{t'} \\
                \end{array} \right. \\ 
p(\emptyset \mid \mathbf{x}) &= \gamma_T(\emptyset_b) \\ 
p(\emptyset \dotsc \mid \mathbf{x}) &= 1 - p(\emptyset \mid \mathbf{x}) \\                             
\end{split} 
 %]]></script>

<p>and iterate util $\max_p p(\mathbf{p} \dotsc \mid \mathbf{x}) &lt; \max_{p’} p(\mathbf{p}’ \mid \mathbf{x})$.</p>

<p>Given enough time, prefix search decoding always finds the most probable labelling. 
However, the maximum number of prefixes it must expand grows exponentially with the input sequence length. 
We need further heuristic.</p>

<p>Observing that the outputs of a trained CTC network tend to form a series of spikes separated by strongly predicted blanks, 
we can divide the output sequence into sections that are very likely to begin and end with a blank. 
We can do this by choosing boundary points where the probability of observing a blank label is above a certain threshold, 
then apply the above algorithm to each section individually and concatenate these to get the final transcription.</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/05/rnnlib-softmax-layer/">RNNLIB: Softmax Layer</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-02-05T21:08:17+08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>9:08 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><ul id="markdown-toc">
  <li><a href="#fundamentals">Fundamentals</a>    <ul>
      <li><a href="#list-of-symbols">List of Symbols</a></li>
      <li><a href="#formulas">Formulas</a></li>
      <li><a href="#layers-in-rnnlib">Layers in RNNLIB</a></li>
    </ul>
  </li>
  <li><a href="#forward-pass">Forward Pass</a></li>
  <li><a href="#backpropagating">Backpropagating</a></li>
</ul>

<p>I used to think that, in order to get the proper gradient, we have to take derivative of 
$\log$ of softmax with respect to weights. However,
the RNNLIB shows that we can actually factorize the network into single layers. In this post, 
we look into the Softmax Layer.</p>

<h2 id="fundamentals">Fundamentals</h2>

<h3 id="list-of-symbols">List of Symbols</h3>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$J$</td>
      <td>cost function</td>
    </tr>
    <tr>
      <td>$y_k$</td>
      <td>activation of a neon</td>
    </tr>
    <tr>
      <td>$u_k$</td>
      <td>input of a neon</td>
    </tr>
    <tr>
      <td>$S_i(\mathbf{u})$</td>
      <td>softmax function, $i$th value for a vector $\mathbf{u}$</td>
    </tr>
  </tbody>
</table>

<h3 id="formulas">Formulas</h3>

<p>Softmax of a vector $\mathbf{u}$ is defined as,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:softmax}
S_i(\mathbf{u}) = \frac{e^{u_i}}{\sum_k{e^{u_k}}} = y_i
\end{equation}</script>

<p>the derivative of softmax is,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} \label{eq:softmax_dev}
    \frac{\partial S_i(\mathbf{u})}{\partial u_j} =
    \frac{\partial y_i}{\partial u_j} = 
          \left\{\begin{array}{ll}
                        y_i(1-y_i) & i = j \\
                        -y_iy_j & i \neq j 
                \end{array} \right.
\end{equation} %]]></script>

<h3 id="layers-in-rnnlib">Layers in RNNLIB</h3>

<p>Every layer in RNNLIB consists of input and output sides,
both sides contain activations and errors.
Their relations with terms in math are shown in following table,</p>

<table>
  <thead>
    <tr>
      <th>Variable</th>
      <th style="text-align: center">Term</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>inputActivations</em></td>
      <td style="text-align: center">$u_k$</td>
    </tr>
    <tr>
      <td><em>outputActivations</em></td>
      <td style="text-align: center">$y_k$</td>
    </tr>
    <tr>
      <td><em>inputErrors</em></td>
      <td style="text-align: center">$\frac{\partial J}{\partial u_k}$</td>
    </tr>
    <tr>
      <td><em>outputErrors</em></td>
      <td style="text-align: center">$\frac{\partial J}{\partial y_k}$</td>
    </tr>
  </tbody>
</table>

<h2 id="forward-pass">Forward Pass</h2>

<p>Forward pass computes $y_k$ from $u_k$ using equation 
\eqref{eq:softmax}. There is a trick in the code, 
we can call it the <em>safe</em> softmax.</p>

<p>To understand it, consider dividing both numerator and denominator
by $e^c$ in equation \eqref{eq:softmax}, </p>

<script type="math/tex; mode=display">\begin{equation}
S_i(\mathbf{u}) 
= \frac{\frac{e^{u_i}}{e^{c}}}{\frac{\sum_k{e^{u_k}}}{e^c}} 
= \frac{e^{u_i - c}}{\sum_k{e^{u_k - c}}} 
= S_i(\hat{\mathbf{u}})  
\end{equation}</script>

<p>thus, in order to avoid overflow when calculating exponentials<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, 
we can replace $u_k$ with $\hat{u}_k=u_k-c$. Typically, $c$ is set to $u_{max}$.</p>

<p>In RNNLIB, <script type="math/tex">c=\frac{u\_{max}+u\_{min}}{2}</script>.</p>

<h2 id="backpropagating">Backpropagating</h2>

<p>Backpropagation computes $\frac{\partial J}{\partial u_k}$ 
from $\frac{\partial J}{\partial y_k}$.</p>

<p>In RNNLIB, the result is</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:error_u_res}
\frac{\partial J}{\partial u_j} = y_j (\frac{\partial J}{\partial y_j} 
- \langle \mathbf{y}, \frac{\partial J}{\partial \mathbf{y}} \rangle)
\end{equation}</script>

<p>where, $\langle \cdot \, , \cdot \rangle$ denotes inner product.</p>

<p>To get the above equation, we first notice that variations in 
$u_j$ give rise to variations in the error function $J$ 
through variations in all $y_k$s. 
Thus, according to the <a href="https://www.math.hmc.edu/calculus/tutorials/multichainrule/">Multivariable Chain Rules</a>,
we can write,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:error_u}
\frac{\partial J}{\partial u_j} = \sum_k{\frac{\partial J}{\partial y_k}\frac{\partial y_k}{\partial u_j}}
\end{equation}</script>

<p>Using equation \eqref{eq:softmax_dev} to replace $\frac{\partial y_k}{\partial u_j}$, we get,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} 
［
\frac{\partial J}{\partial u_j} &= y_j(1-y_j)\frac{\partial J}{\partial y_j} +
\sum_{k: k\neq j}{-y_k y_j \frac{\partial J}{\partial y_k}} \\
&= y_j(\frac{\partial J}{\partial y_j} -y_j \frac{\partial J}{\partial y_j} 
+ \sum_{k: k\neq j}{-y_k \frac{\partial J}{\partial y_k}}) \\
&= y_j(\frac{\partial J}{\partial y_j} - \sum_{k}{y_k \frac{\partial J}{\partial y_k}}) \\
&= y_j(\frac{\partial J}{\partial y_j} - \langle \mathbf{y}, \frac{\partial J}{\partial \mathbf{y}} \rangle)
\end{split}
\end{equation} %]]></script>

<p>Finally, we reach equation \eqref{eq:error_u_res}.</p>

<p>In this way, softmax operation can be implemented to be a standalone layer.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Strictly speaking, this converts overflow into underflow. 
  Underflow is no problem, because that rounds off to zero, which is a well-behaved floating point number.
  otherwise, it will be Infinity or NaN. see <a href="http://lingpipe-blog.com/2009/03/17/softmax-without-overflow/">this article</a> for details. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/05/rnnlib-introduction/">RNNLIB: Introduction</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-02-05T16:02:28+08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>4:02 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>RNNLIB is a recurrent neural network library for sequence learning problems,
which is written by <a href="http://www.cs.toronto.edu/~graves/">Alex Graves</a>.</p>

<p>In <a href="http://www6.in.tum.de/pub/Main/Publications/Graves2006a.pdf">this paper</a>,
Graves proposed the CTC(Connectionist Temporal Classification), 
which allows the system to transcribe unsegmented sequence data. 
The most exciting thing is that by training a deep bidirectional 
LSTM network with CTC, it is possible to 
perform automatic speech recognition in 
an <a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf">end-to-end fashion</a>, 
i.e. without any human expertise.</p>

<p>RNNLIB covers all the theories in Graves’s paper, including:</p>

<ul>
  <li>Bidirectional Long Short-Term Memory</li>
  <li>Connectionist Temporal Classification</li>
  <li>Multidimensional Recurrent Neural Networks</li>
</ul>

<p>I will try to explain the codes in RNNLIB in following posts.</p>

<ol>
  <li><a href="/blog/2015/02/05/rnnlib-softmax-layer/">RNNLIB: Softmax Layer</a> </li>
  <li><a href="/blog/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/">RNNLIB: Connectionist Temporal Classification and Transcription Layer</a></li>
</ol>

</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/03/03/dropout-training/">Dropout Training</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/">RNNLIB: Connectionist Temporal Classification and Transcription Layer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/05/rnnlib-softmax-layer/">RNNLIB: Softmax Layer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/05/rnnlib-introduction/">RNNLIB: Introduction</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/wantee">@wantee</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'wantee',
            count: 0,
            skip_forks: true,
            skip_repos: [ "wantee.github.io" ],
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Wantee Wang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'wantee';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>






  <!-- mathjax config similar to math.stackexchange -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$']],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</body>
</html>
