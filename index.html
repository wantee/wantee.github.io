
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Formula Coding</title>
  <meta name="author" content="Wantee Wang">

  
  <meta name="description" content="The Name The Theory List of Symbols Training Procedure The CTC Forward-Backward Algorithm The Implementation CTC is the core concept make it &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://wantee.github.io/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Formula Coding" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-58316654-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Formula Coding</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="wantee.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/">RNNLIB: Connectionist Temporal Classification and Transcription Layer</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-02-08T16:56:40+08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>8</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>4:56 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><ul id="markdown-toc">
  <li><a href="#the-name">The Name</a></li>
  <li><a href="#the-theory">The Theory</a>    <ul>
      <li><a href="#list-of-symbols">List of Symbols</a></li>
      <li><a href="#training-procedure">Training Procedure</a></li>
      <li><a href="#the-ctc-forward-backward-algorithm">The CTC Forward-Backward Algorithm</a></li>
    </ul>
  </li>
  <li><a href="#the-implementation">The Implementation</a></li>
</ul>

<p>CTC is the core concept make it possible to transcribe unsegmented sequence data.
RNNLIB implements it in a single layer called Transcription Layer.
We go into this particular layer in this post, the main reference is the Graves’
<a href="http://www6.in.tum.de/pub/Main/Publications/Graves2006a.pdf">original paper</a>.</p>

<p>The key point for CTC is to use a simple map transforming the RNN output to unsegmented labelling,
and construct a new objective function based on the map.
This map do not need a precise alignment, thus greatly simplify the task and reduce human expert involvement. </p>

<h2 id="the-name">The Name</h2>

<p>“Connectionist” is the adjective form of “connectionism”, 
<a href="http://en.wikipedia.org/wiki/Connectionism">Connectionism</a> is a terminology in cognitive science,
which models mental or behavioural phenomena as the emergent processes of interconnected networks of simple units.
The most common forms use neural network models. </p>

<p>In the traditional neural network recipe, we independently model the input sequence 
in each time-step or frame. This can be referred as <em>framewise classification</em>.
<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.8007&amp;rep=rep1&amp;type=pdf">Kadous</a> 
extends the classification paradigm to multivariate time series, and 
names it as <em>temporal classification</em>.
With this, we do not have to label every time step in training data set.</p>

<p>Combining RNN and temporal classification, Graves proposes the <em>connectionist temporal classification</em>.</p>

<p>To distinguish from classification, RNNLIB implements the CTC as <em>Transcription Layer</em>, 
indicating that with CTC we can directly transcribe input sequence(e.g. acoustic signal)
into output sequence(e.g. words).</p>

<h2 id="the-theory">The Theory</h2>

<h3 id="list-of-symbols">List of Symbols</h3>

<p>Following the notations in the paper, we first list the symbols.</p>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$L$</td>
      <td>(finite) alphabet of labels</td>
    </tr>
    <tr>
      <td>$L’$</td>
      <td>$L \cup \{blank\}$</td>
    </tr>
    <tr>
      <td>$\mathcal{X}$</td>
      <td>$(\mathbb{R}^m)^{*}$, $m$ dimensional input space</td>
    </tr>
    <tr>
      <td>$\mathcal{Z}$</td>
      <td>$L^{*}$, output space, set of all sequences over the $L$</td>
    </tr>
    <tr>
      <td>$\mathcal{D_{X \times Z}}$</td>
      <td>underlying distribution of data</td>
    </tr>
    <tr>
      <td>$S$</td>
      <td>set of training examples supposed to be drawn from $\mathcal{D_{X \times Z}}$</td>
    </tr>
    <tr>
      <td>($\mathbf{x},\mathbf{z})$</td>
      <td>example in $S$, $\mathbf{x} = (x_1, x_2, \dotsc, x_T)$, $\mathbf{z} = (z_1, z_2, \dotsc, z_U)$ and $U \leq T$</td>
    </tr>
    <tr>
      <td>$h:\mathcal{X} \mapsto \mathcal{Z}$</td>
      <td>temporal classifier to be trained</td>
    </tr>
    <tr>
      <td>$\mathcal{N}_{w}:(R^{m})^{T} \mapsto (R^n)^{T}$</td>
      <td>RNN, with $m$ inputs, $n$ outputs and weight vector $w$, as a continuous map</td>
    </tr>
    <tr>
      <td>$\mathbf{y} = \mathcal{N}_{w}$</td>
      <td>sequence of RNN output</td>
    </tr>
    <tr>
      <td>$y_{k}^{t}$</td>
      <td>the activation of output unit $k$ at time $t$</td>
    </tr>
    <tr>
      <td>$\pi$</td>
      <td><em>path</em>, element of $L’^{T}$</td>
    </tr>
    <tr>
      <td>$\mathbf{l} \in L^{\leq T}$</td>
      <td>label sequence or <em>labelling</em></td>
    </tr>
    <tr>
      <td>$\mathcal{B}:L’^{T} \mapsto L^{\leq T}$</td>
      <td>map from path to labelling</td>
    </tr>
    <tr>
      <td>$\mathbf{l}_{a\mathord{:}b}$</td>
      <td>sub-sequence of $\mathbf{l}$ from $a$th to $b$th labels</td>
    </tr>
    <tr>
      <td>$\mathbf{l}’$</td>
      <td>modified label sequence, with blanks added to the beginning and the end and inserted between every pair of labels in $\mathbf{l}$</td>
    </tr>
    <tr>
      <td>$\alpha_t(s)$</td>
      <td>forward variable, the total probability of $\mathbf{l}_{1:s}$ at time $t$</td>
    </tr>
    <tr>
      <td>$\beta_t(s) $</td>
      <td>backward variable, the total probability of $\mathbf{l}_{s:|\mathbf{l}’|}$ at time $t$</td>
    </tr>
    <tr>
      <td>$\tilde{\beta}_t(s) $</td>
      <td>backward variable, the total probability of $\mathbf{l}_{s:|\mathbf{l}’|}$ start at time $t+1$</td>
    </tr>
    <tr>
      <td>$O^{ML}(S,\mathcal{N}_{w})$</td>
      <td>maximum likelihood objective function</td>
    </tr>
    <tr>
      <td>$\delta_{kk’}$</td>
      <td><a href="http://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a></td>
    </tr>
  </tbody>
</table>

<h3 id="training-procedure">Training Procedure</h3>

<p>The goal is to use $S$ to train a temporal classifier $h$ to classify previously unseen input sequences in a way that minimises the ML objective function:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:obj_ml}
O^{ML}(S,\mathcal{N}_{w}) = - \sum_{(\mathbf{x},\mathbf{z})\in S}{\ln(p(\mathbf{z}|\mathbf{x}))}
\end{equation}</script>

<p>To train the network with gradient descent, 
we need to differentiate \eqref{eq:obj_ml} with respect to the network outputs. 
Since the training examples are independent we can consider them separately:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:obj}
\frac{\partial O^{ML}(\{(\mathbf{x},\mathbf{z}\},\mathcal{N}_{w})}{\partial y_k^t} 
    = - \frac{\partial \ln(p(\mathbf{z}|\mathbf{x}))}{\partial y_k^t}
    = - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{\partial p(\mathbf{z}|\mathbf{x})}{\partial y^t_k}
\end{equation}</script>

<p>Another thing we have to consider is how to map from network outputs to labellings.
Use $\mathcal{B}$ to denote such a map. Given a path, we simply removing all blanks 
and repeated labels and the remaining labels form a labelling(e.g. $\mathcal{B}(a-ab-)=\mathcal{B}(-aa–abb)=aab$). </p>

<p>Then we can define the conditional probability of a labelling,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:labelling}
p(\mathbf{l}|\mathbf{x}) = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{l})}{p(\pi|\mathbf{x})}
\end{equation}</script>

<p>where, $p(\pi|\mathbf{x})$ is the conditional probability of a path given $\mathbf{x}$, and is defined as:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:path}
p(\pi|\mathbf{x}) = \prod_{t=1}^{T}{y_{\pi_t}^{t}},\forall \pi \in L'^{T}
\end{equation}</script>

<p>To calculate \eqref{eq:obj}, we first define the forward and backward variable,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:fwd}
\alpha_t(s) = \sum_{\pi \in L^{T}:\mathcal{B}(\pi_{1\mathord{:}t})=\mathbf{l}_{1\mathord{:}s}}
   {\prod_{t'=1}^{t}{y^{t'}_{\pi_{t'}}}}
\end{equation}</script>

<script type="math/tex; mode=display">\begin{equation} \label{eq:bwd}
\beta_t(s) = \sum_{\pi \in L^{T}:\mathcal{B}(\pi_{t\mathord{:}T})=\mathbf{l}_{s\mathord{:} |\mathbf{l}'|}}
   {\prod_{t'=t}^{T}{y^{t'}_{\pi_{t'}}}}
\end{equation}</script>

<p>Note that the product of the forward and backward variables at a given $s$ and $t$ is the probability of all the paths corresponding to $\mathbf{l}$ that go through the symbol $s$ at time $t$, i.e.,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:fwd_bwd_ori}
\alpha_t(s)\beta_t(s) = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{l}):\pi_{t}=\mathbf{l}'_{s}}
   {y^{t}_{\mathbf{l}'_{s}}\prod_{t=1}^{T}{y^{t}_{\pi_{t}}}}
\end{equation}</script>

<p>Rearranging and substituting in from \eqref{eq:path} gives,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:fwd_bwd}
\frac{\alpha_t(s)\beta_t(s)}{y^{t}_{\mathbf{l}'_{s}}} = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{l}):\pi_{t}=\mathbf{l}'_{s}}
   {p(\pi|\mathbf{x})}
\end{equation}</script>

<p>For any $t$, we can therefore sum over all $s$ to get $p(\mathbf{l} | \mathbf{x})$:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:labelling_fwd_bwd}
p(\mathbf{l}|\mathbf{x}) = \sum_{s=1}^{|\mathbf{l}'|}\frac{\alpha_t(s)\beta_t(s)}{y^{t}_{\mathbf{l}'_{s}}}
\end{equation}</script>

<p>On the other hand, combining \eqref{eq:labelling} and \eqref{eq:path},</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:labelling_all}
p(\mathbf{l}|\mathbf{x}) = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{l})}{\prod_{t=1}^{T}{y_{\pi_t}^{t}}}
\end{equation}</script>

<p>Thus to differentiate this with respect to $y_k^t$ , 
we need only consider those paths going through label $k$ at time $t$
(derivatives of other paths is zero). 
Noting that the same label (or blank) may be repeated several times for a single labelling $\mathbf{l}$, 
we define the set of positions where label $k$ occurs as $lab(\mathbf{l},k) = \{s : \mathbf{l}’_s = k\}$, 
which may be empty. We then differentiate \eqref{eq:labelling_all} to get,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} \label{eq:labelling_drv}
\begin{split} 
\frac{\partial p(\mathbf{l}|\mathbf{x})}{\partial y_k^t} 
  &= \sum_{s \in lab(\mathbf{l}, k)}{\frac{\partial p(\pi|\mathbf{x})}{\partial y_k^t}} \\
  &= \sum_{s \in lab(\mathbf{l}, k)}{\frac{\partial \prod_{t'=1}^{T}{y_{\pi_{t'}}^{t'}}}{\partial y_k^t}} \\
  &= \sum_{s \in lab(\mathbf{l}, k)}{\frac{\partial y_k^t\prod_{t' \neq t}{y_{\pi_{t'}}^{t'}}}{\partial y_k^t}} \\
  &= \sum_{s \in lab(\mathbf{l}, k)}{\prod_{t' \neq t}{y_{\pi_{t'}}^{t'}}} \\
  &= \frac{1}{ {y^t_k}^2 } \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\beta_t(s)}
\end{split} 
\end{equation} %]]></script>

<p>At this point, we can set $\mathbf{l} = \mathbf{z}$ and substituting into \eqref{eq:obj}, then get the final gradient,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:grad}
\frac{\partial O^{ML}(\{(\mathbf{x},\mathbf{z}\},\mathcal{N}_{w})}{\partial y_k^t} 
    = - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ {y^t_k}^2 } \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\beta_t(s)}
\end{equation}</script>

<p>where, $p(\mathbf{z}|\mathbf{x})$ can be calculated from \eqref{eq:labelling_fwd_bwd}.</p>

<p>Next, we can give the gradient for the unnormalised output $u_k^t$. Recall that derivative of softmax function is, </p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:err_softmax}
\frac{\partial y^t_{k'}}{\partial u_k^t} = y^t_{k'}\delta_{kk'} - y^t_{k'}y^t_k
\end{equation}</script>

<p>Then we get,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} \label{eq:error_u}
\begin{split} 
\frac{\partial O}{\partial u_k^t} 
  &= \sum_{k'}{\frac{\partial O}{\partial y^t_{k'}}\frac{\partial y^t_{k'}}{\partial u^t_k}} \\
  &= \sum_{k'}{((y^t_{k'}\delta_{kk'} - y^t_{k'}y^t_k) (-\frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ {y^t_{k'}}^2 } 
                \sum_{s \in lab(\mathbf{l}, k')}{\alpha_t(s)\beta_t(s)}))} \\
  &= \sum_{k'}{(\frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{y^t_{k}}{ {y^t_{k'}} } 
                \sum_{s \in lab(\mathbf{l}, k')}{\alpha_t(s)\beta_t(s)})} - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ {y^t_k} } 
                \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\beta_t(s)}\\  
  &= y^t_k - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ {y^t_k} } 
                \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\beta_t(s)}\\                               
\end{split} 
\end{equation} %]]></script>

<p>we write the last step by noting that $\sum_{k’}\sum_{s \in lab(\mathbf{l}, k’)}{(\cdot)} \equiv \sum_{s=1}^{|\mathbf{l}’|}{(\cdot)}$,
then, using \eqref{eq:labelling_fwd_bwd}, the $p(\mathbf{z}|\mathbf{x})$ is canceled out.</p>

<h3 id="the-ctc-forward-backward-algorithm">The CTC Forward-Backward Algorithm</h3>

<p>The last thing we have to do is calculating the forward and backward variables. We now show that by define a recursive from, 
these variables can be calculated efficiently.</p>

<p>Given a labelling $\mathbf{l}$, we first extend it to $\mathbf{l}’$ with blanks added to the beginning 
and the end and inserted between every pair of labels. 
The length of $\mathbf{l}’$ is therefore $2|\mathbf{l}| + 1$. 
In calculating the probabilities of prefixes of $\mathbf{l}’$ we allow all transitions between blank and non-blank labels, 
and also those between any pair of distinct non-blank labels(because of the map $\mathcal{B}$, the repeated labels will be merged). 
We allow all prefixes to start with either a blank ($b$) or the first symbol in $\mathbf{l}$ ($\mathbf{l}_1$).</p>

<p>This gives us the following rules for initialisation</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{split} 
\alpha_1(1) &= y_b^1 \\ 
\alpha_1(2) &= y_{\mathbf{l}_1}^1 \\ 
\alpha_1(s) &= 0, \forall s > 2 \\                               
\end{split} 
 %]]></script>

<p>and recursion</p>

<script type="math/tex; mode=display">% <![CDATA[
 \begin{equation} \label{eq:alpha}
    \alpha_t(s) = \left\{\begin{array}{ll}
                                    y_{\mathbf{l}'_s}^t(\alpha_{t-1}(s) + \alpha_{t-1}(s-1)) & \mathbf{l}'_s = b\, \text{or} \, \mathbf{l}'_{s-2} = \mathbf{l}'_s\\
                                    y_{\mathbf{l}'_s}^t(\alpha_{t-1}(s) + \alpha_{t-1}(s-1) + \alpha_{t-1}(s-2)) & otherwise
                                \end{array} \right.
\end{equation} %]]></script>

<p>Note that $\alpha_t(s) = 0, \forall s &lt; |\mathbf{l}’|−2(T −t)−1$, 
because these variables correspond to states for which there are not enough time-steps left to complete the sequence.</p>

<p>Here we can get another method to calculate $p(\mathbf{l} | \mathbf{x})$, by adding up all forward variables at time $T$, i.e.,</p>

<script type="math/tex; mode=display"> \begin{equation} \label{eq:porb}
    p(\mathbf{l} | \mathbf{x}) = \alpha_T(|\mathbf{l}'|) + \alpha_T(|\mathbf{l}'| - 1)
\end{equation}</script>

<p>Similarly, the backward variables can be initalisd as,</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{split} 
\beta_T(|\mathbf{l}'|) &= y_b^T \\ 
\beta_T(|\mathbf{l}'| - 1) &= y_{\mathbf{l}_{|\mathbf{l}|}}^T \\ 
\beta_T(s) &= 0, \forall s< |\mathbf{l}'| - 1\\                               
\end{split} 
 %]]></script>

<p>and recursion</p>

<script type="math/tex; mode=display">% <![CDATA[
 \begin{equation} \label{eq:beta}
    \beta_t(s) = \left\{\begin{array}{ll}
                y_{\mathbf{l}'_s}^t(\beta_{t+1}(s) + \beta_{t+1}(s+1)) & \mathbf{l}'_s = b\, \text{or} \, \mathbf{l}'_{s+2} = \mathbf{l}'_s\\
                y_{\mathbf{l}'_s}^t(\beta_{t+1}(s) + \beta_{t+1}(s+1) + \beta_{t+1}(s+2)) & otherwise
                  \end{array} \right.
\end{equation} %]]></script>

<p>Note that $\beta_t(s) = 0, \forall s &gt; 2t$.</p>

<p>Following figure illustrate the forward backward algorithm applied to the labelling ‘CAT’(from the paper).</p>

<p><img class="center" src="/images/posts/CTC-alpha-beta.png" title="Alpha-Beta Algorithm" /></p>

<h2 id="the-implementation">The Implementation</h2>

<p>The <code>TranscriptionLayer</code> class inherits the <code>SoftmaxLayer</code> class(see <a href="/blog/2015/02/05/rnnlib-softmax-layer/">this post</a>).
The <code>feed_forward()</code> and <code>feed_back()</code> methods are the general softmax function, 
so only need to implement the <code>calculate_errors()</code> method to calculate the $\frac{\partial O}{\partial y_k^t}$.
In order to use \eqref{eq:grad} to get output error, first need to calculate the $\alpha$s and $\beta$s.
Forward variables are got using \eqref{eq:alpha}. </p>

<p>But backward variables are in another form, given in Graves’ <a href="www6.in.tum.de/Main/Publications/Graves2008c.pdf">Dissertation</a>.
Consider backward variable started from time $t+1$,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:bwd_new}
\tilde\beta_t(s) = \sum_{\pi \in L^{T}:\mathcal{B}(\pi_{t\mathord{:}T})=\mathbf{l}_{s\mathord{:} |\mathbf{l}'|}}
   {\prod_{t'=t+1}^{T}{y^{t'}_{\pi_{t'}}}}
\end{equation}</script>

<p>Noting that, $\beta$ and $\tilde\beta$ has a simple relationship:</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:bwd_relaion}
\beta_t(s) = y_{\pi_{t}}^t\tilde\beta_t(s)
\end{equation}</script>

<p>Thus, we can get recursion formula for $\tilde\beta$ by substituting \eqref{eq:bwd_relaion} into \eqref{eq:beta},</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{split} 
\tilde\beta_T(|\mathbf{l}'|) &= 1 \\ 
\tilde\beta_T(|\mathbf{l}'| - 1) &= 1 \\ 
\tilde\beta_T(s) &= 0, \forall s< |\mathbf{l}'| - 1\\                               
\end{split} 
 %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
 \begin{equation} \label{eq:beta_new}
    \tilde\beta_t(s) = \left\{\begin{array}{ll}
                y_{\mathbf{l}'_s}^{t+1}\tilde\beta_{t+1}(s) + y_{\mathbf{l}'_{s+1}}^{t+1}\tilde\beta_{t+1}(s+1) & \mathbf{l}'_s = b\, \text{or} \, \mathbf{l}'_{s+2} = \mathbf{l}'_s\\
                y_{\mathbf{l}'_s}^{t+1}\tilde\beta_{t+1}(s) + y_{\mathbf{l}'_{s+1}}^{t+1}\tilde\beta_{t+1}(s+1) + y_{\mathbf{l}'_{s+2}}^{t+1}\tilde\beta_{t+1}(s+2) & otherwise
                \end{array} \right.
\end{equation} %]]></script>

<p>Noting that, if $\mathbf{l}’_s \neq blank$, then $\mathbf{l}’_{s+1}$ must be $blank$.</p>

<p>And the gradient for output \eqref{eq:grad} becomes,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:grad_new}
\frac{\partial O^{ML}(\{(\mathbf{x},\mathbf{z}\},\mathcal{N}_{w})}{\partial y_k^t} 
    = - \frac{1}{p(\mathbf{z}|\mathbf{x})} \frac{1}{ y^t_k } \sum_{s \in lab(\mathbf{l}, k)}{\alpha_t(s)\tilde\beta_t(s)}
\end{equation}</script>

<p>where,</p>

<script type="math/tex; mode=display">\begin{equation}
p(\mathbf{z}|\mathbf{x}) = \sum_{s=1}^{|\mathbf{z}'|}{\alpha_t(s)\tilde\beta_t(s)}
\end{equation}</script>

<p>Actually, the RNNLIB code computes $p(\mathbf{z}|\mathbf{x})$ using \eqref{eq:porb}.</p>

<p>To wrap up, CTC using a forward-backward algorithm to efficiently compute the RNN output errors, 
corresponding to a new ML objective function. With these errors, 
we can use any traditional gradient methods to train the network.</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/05/rnnlib-softmax-layer/">RNNLIB: Softmax Layer</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-02-05T21:08:17+08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>9:08 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><ul id="markdown-toc">
  <li><a href="#fundamentals">Fundamentals</a>    <ul>
      <li><a href="#list-of-symbols">List of Symbols</a></li>
      <li><a href="#formulas">Formulas</a></li>
      <li><a href="#layers-in-rnnlib">Layers in RNNLIB</a></li>
    </ul>
  </li>
  <li><a href="#forward-pass">Forward Pass</a></li>
  <li><a href="#backpropagating">Backpropagating</a></li>
</ul>

<p>I used to think that, in order to get the proper gradient, we have to take derivative of 
$\log$ of softmax with respect to weights. However,
the RNNLIB shows that we can actually factorize the network into single layers. In this post, 
we look into the Softmax Layer.</p>

<h2 id="fundamentals">Fundamentals</h2>

<h3 id="list-of-symbols">List of Symbols</h3>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$J$</td>
      <td>cost function</td>
    </tr>
    <tr>
      <td>$y_k$</td>
      <td>activation of a neon</td>
    </tr>
    <tr>
      <td>$u_k$</td>
      <td>input of a neon</td>
    </tr>
    <tr>
      <td>$S_i(\mathbf{u})$</td>
      <td>softmax function, $i$th value for a vector $\mathbf{u}$</td>
    </tr>
  </tbody>
</table>

<h3 id="formulas">Formulas</h3>

<p>Softmax of a vector $\mathbf{u}$ is defined as,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:softmax}
S_i(\mathbf{u}) = \frac{e^{u_i}}{\sum_k{e^{u_k}}} = y_i
\end{equation}</script>

<p>the derivative of softmax is,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} \label{eq:softmax_dev}
    \frac{\partial S_i(\mathbf{u})}{\partial u_j} =
    \frac{\partial y_i}{\partial u_j} = 
          \left\{\begin{array}{ll}
                        y_i(1-y_i) & i = j \\
                        -y_iy_j & i \neq j 
                \end{array} \right.
\end{equation} %]]></script>

<h3 id="layers-in-rnnlib">Layers in RNNLIB</h3>

<p>Every layer in RNNLIB consists of input and output sides,
both sides contain activations and errors.
Their relations with terms in math are shown in following table,</p>

<table>
  <thead>
    <tr>
      <th>Variable</th>
      <th style="text-align: center">Term</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>inputActivations</em></td>
      <td style="text-align: center">$u_k$</td>
    </tr>
    <tr>
      <td><em>outputActivations</em></td>
      <td style="text-align: center">$y_k$</td>
    </tr>
    <tr>
      <td><em>inputErrors</em></td>
      <td style="text-align: center">$\frac{\partial J}{\partial u_k}$</td>
    </tr>
    <tr>
      <td><em>outputErrors</em></td>
      <td style="text-align: center">$\frac{\partial J}{\partial y_k}$</td>
    </tr>
  </tbody>
</table>

<h2 id="forward-pass">Forward Pass</h2>

<p>Forward pass computes $y_k$ from $u_k$ using equation 
\eqref{eq:softmax}. There is a trick in the code, 
we can call it the <em>safe</em> softmax.</p>

<p>To understand it, consider dividing both numerator and denominator
by $e^c$ in equation \eqref{eq:softmax}, </p>

<script type="math/tex; mode=display">\begin{equation}
S_i(\mathbf{u}) 
= \frac{\frac{e^{u_i}}{e^{c}}}{\frac{\sum_k{e^{u_k}}}{e^c}} 
= \frac{e^{u_i - c}}{\sum_k{e^{u_k - c}}} 
= S_i(\hat{\mathbf{u}})  
\end{equation}</script>

<p>thus, in order to avoid overflow when calculating exponentials<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, 
we can replace $u_k$ with $\hat{u}_k=u_k-c$. Typically, $c$ is set to $u_{max}$.</p>

<p>In RNNLIB, <script type="math/tex">c=\frac{u\_{max}+u\_{min}}{2}</script>.</p>

<h2 id="backpropagating">Backpropagating</h2>

<p>Backpropagation computes $\frac{\partial J}{\partial u_k}$ 
from $\frac{\partial J}{\partial y_k}$.</p>

<p>In RNNLIB, the result is</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:error_u_res}
\frac{\partial J}{\partial u_j} = y_j (\frac{\partial J}{\partial y_j} 
- \langle \mathbf{y}, \frac{\partial J}{\partial \mathbf{y}} \rangle)
\end{equation}</script>

<p>where, $\langle \cdot \, , \cdot \rangle$ denotes inner product.</p>

<p>To get the above equation, we first notice that variations in 
$u_j$ give rise to variations in the error function $J$ 
through variations in all $y_k$s. 
Thus, according to the <a href="https://www.math.hmc.edu/calculus/tutorials/multichainrule/">Multivariable Chain Rules</a>,
we can write,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:error_u}
\frac{\partial J}{\partial u_j} = \sum_k{\frac{\partial J}{\partial y_k}\frac{\partial y_k}{\partial u_j}}
\end{equation}</script>

<p>Using equation \eqref{eq:softmax_dev} to replace $\frac{\partial y_k}{\partial u_j}$, we get,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} 
［
\frac{\partial J}{\partial u_j} &= y_j(1-y_j)\frac{\partial J}{\partial y_j} +
\sum_{k: k\neq j}{-y_k y_j \frac{\partial J}{\partial y_k}} \\
&= y_j(\frac{\partial J}{\partial y_j} -y_j \frac{\partial J}{\partial y_j} 
+ \sum_{k: k\neq j}{-y_k \frac{\partial J}{\partial y_k}}) \\
&= y_j(\frac{\partial J}{\partial y_j} - \sum_{k}{y_k \frac{\partial J}{\partial y_k}}) \\
&= y_j(\frac{\partial J}{\partial y_j} - \langle \mathbf{y}, \frac{\partial J}{\partial \mathbf{y}} \rangle)
\end{split}
\end{equation} %]]></script>

<p>Finally, we reach equation \eqref{eq:error_u_res}.</p>

<p>In this way, softmax operation can be implemented to be a standalone layer.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Strictly speaking, this converts overflow into underflow. 
  Underflow is no problem, because that rounds off to zero, which is a well-behaved floating point number.
  otherwise, it will be Infinity or NaN. see <a href="http://lingpipe-blog.com/2009/03/17/softmax-without-overflow/">this article</a> for details. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/05/rnnlib-introduction/">RNNLIB: Introduction</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-02-05T16:02:28+08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>4:02 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>RNNLIB is a recurrent neural network library for sequence learning problems,
which is written by <a href="http://www.cs.toronto.edu/~graves/">Alex Graves</a>.</p>

<p>In <a href="http://www6.in.tum.de/pub/Main/Publications/Graves2006a.pdf">this paper</a>,
Graves proposed the CTC(Connectionist Temporal Classification), 
which allows the system to transcribe unsegmented sequence data. 
The most exciting thing is that by training a deep bidirectional 
LSTM network with CTC, it is possible to 
perform automatic speech recognition in 
an <a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf">end-to-end fashion</a>, 
i.e. without any human expertise.</p>

<p>RNNLIB covers all the theories in Graves’s paper, including:</p>

<ul>
  <li>Bidirectional Long Short-Term Memory</li>
  <li>Connectionist Temporal Classification</li>
  <li>Multidimensional Recurrent Neural Networks</li>
</ul>

<p>I will try to explain the codes in RNNLIB in following posts.</p>

<ol>
  <li><a href="/blog/2015/02/05/rnnlib-softmax-layer/">RNNLIB: Softmax Layer</a> </li>
  <li><a href="/blog/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/">RNNLIB: Connectionist Temporal Classification and Transcription Layer</a></li>
</ol>

</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/">RNNLIB: Connectionist Temporal Classification and Transcription Layer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/05/rnnlib-softmax-layer/">RNNLIB: Softmax Layer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/05/rnnlib-introduction/">RNNLIB: Introduction</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/wantee">@wantee</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'wantee',
            count: 0,
            skip_forks: true,
            skip_repos: [ "wantee.github.io" ],
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Wantee Wang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'wantee';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>






  <!-- mathjax config similar to math.stackexchange -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$']],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</body>
</html>
