
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>RNNLIB: Softmax Layer - Formula Coding</title>
  <meta name="author" content="Wantee Wang">

  
  <meta name="description" content="Fundamentals List of Symbols Formulas Layers in RNNLIB Forward Pass Backpropagating I used to think that, in order to get the proper gradient, we &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://wantee.github.io/blog/2015/02/05/rnnlib-softmax-layer/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Formula Coding" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-58316654-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Formula Coding</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="wantee.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">RNNLIB: Softmax Layer</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-02-05T21:08:17+08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>9:08 pm</span></time>
        <span class="printable">


  


<a href="/assets/printables/2015-02-05-rnnlib-softmax-layer.pdf"> <img src="/images/printer.png" alt="printable version"> </a>

</span>

        
      </p>
    
  </header>


<div class="entry-content"><ul id="markdown-toc">
  <li><a href="#fundamentals">Fundamentals</a>    <ul>
      <li><a href="#list-of-symbols">List of Symbols</a></li>
      <li><a href="#formulas">Formulas</a></li>
      <li><a href="#layers-in-rnnlib">Layers in RNNLIB</a></li>
    </ul>
  </li>
  <li><a href="#forward-pass">Forward Pass</a></li>
  <li><a href="#backpropagating">Backpropagating</a></li>
</ul>

<p>I used to think that, in order to get the proper gradient, we have to take derivative of 
$\log$ of softmax with respect to weights. However,
the RNNLIB shows that we can actually factorize the network into single layers. In this post, 
we look into the Softmax Layer.</p>

<h2 id="fundamentals">Fundamentals</h2>

<h3 id="list-of-symbols">List of Symbols</h3>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$J$</td>
      <td>cost function</td>
    </tr>
    <tr>
      <td>$y_k$</td>
      <td>activation of a neon</td>
    </tr>
    <tr>
      <td>$u_k$</td>
      <td>input of a neon</td>
    </tr>
    <tr>
      <td>$S_i(\mathbf{u})$</td>
      <td>softmax function, $i$th value for a vector $\mathbf{u}$</td>
    </tr>
  </tbody>
</table>

<h3 id="formulas">Formulas</h3>

<p>Softmax of a vector $\mathbf{u}$ is defined as,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:softmax}
S_i(\mathbf{u}) = \frac{e^{u_i}}{\sum_k{e^{u_k}}} = y_i
\end{equation}</script>

<p>the derivative of softmax is,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} \label{eq:softmax_dev}
    \frac{\partial S_i(\mathbf{u})}{\partial u_j} =
    \frac{\partial y_i}{\partial u_j} = 
          \left\{\begin{array}{ll}
                        y_i(1-y_i) & i = j \\
                        -y_iy_j & i \neq j 
                \end{array} \right.
\end{equation} %]]></script>

<h3 id="layers-in-rnnlib">Layers in RNNLIB</h3>

<p>Every layer in RNNLIB consists of input and output sides,
both sides contain activations and errors.
Their relations with terms in math are shown in following table,</p>

<table>
  <thead>
    <tr>
      <th>Variable</th>
      <th style="text-align: center">Term</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>inputActivations</em></td>
      <td style="text-align: center">$u_k$</td>
    </tr>
    <tr>
      <td><em>outputActivations</em></td>
      <td style="text-align: center">$y_k$</td>
    </tr>
    <tr>
      <td><em>inputErrors</em></td>
      <td style="text-align: center">$\frac{\partial J}{\partial u_k}$</td>
    </tr>
    <tr>
      <td><em>outputErrors</em></td>
      <td style="text-align: center">$\frac{\partial J}{\partial y_k}$</td>
    </tr>
  </tbody>
</table>

<h2 id="forward-pass">Forward Pass</h2>

<p>Forward pass computes $y_k$ from $u_k$ using equation 
\eqref{eq:softmax}. There is a trick in the code, 
we can call it the <em>safe</em> softmax.</p>

<p>To understand it, consider dividing both numerator and denominator
by $e^c$ in equation \eqref{eq:softmax}, </p>

<script type="math/tex; mode=display">\begin{equation}
S_i(\mathbf{u}) 
= \frac{\frac{e^{u_i}}{e^{c}}}{\frac{\sum_k{e^{u_k}}}{e^c}} 
= \frac{e^{u_i - c}}{\sum_k{e^{u_k - c}}} 
= S_i(\hat{\mathbf{u}})  
\end{equation}</script>

<p>thus, in order to avoid overflow when calculating exponentials<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, 
we can replace $u_k$ with $\hat{u}_k=u_k-c$. Typically, $c$ is set to $u_{max}$.</p>

<p>In RNNLIB, <script type="math/tex">c=\frac{u\_{max}+u\_{min}}{2}</script>.</p>

<h2 id="backpropagating">Backpropagating</h2>

<p>Backpropagation computes $\frac{\partial J}{\partial u_k}$ 
from $\frac{\partial J}{\partial y_k}$.</p>

<p>In RNNLIB, the result is</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:error_u_res}
\frac{\partial J}{\partial u_j} = y_j (\frac{\partial J}{\partial y_j} 
- \langle \mathbf{y}, \frac{\partial J}{\partial \mathbf{y}} \rangle)
\end{equation}</script>

<p>where, $\langle \cdot \, , \cdot \rangle$ denotes inner product.</p>

<p>To get the above equation, we first notice that variations in 
$u_j$ give rise to variations in the error function $J$ 
through variations in all $y_k$s. 
Thus, according to the <a href="https://www.math.hmc.edu/calculus/tutorials/multichainrule/">Multivariable Chain Rules</a>,
we can write,</p>

<script type="math/tex; mode=display">\begin{equation} \label{eq:error_u}
\frac{\partial J}{\partial u_j} = \sum_k{\frac{\partial J}{\partial y_k}\frac{\partial y_k}{\partial u_j}}
\end{equation}</script>

<p>Using equation \eqref{eq:softmax_dev} to replace $\frac{\partial y_k}{\partial u_j}$, we get,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} 
\begin{split}
\frac{\partial J}{\partial u_j} &= y_j(1-y_j)\frac{\partial J}{\partial y_j} +
\sum_{k: k\neq j}{-y_k y_j \frac{\partial J}{\partial y_k}} \\
&= y_j(\frac{\partial J}{\partial y_j} -y_j \frac{\partial J}{\partial y_j} 
+ \sum_{k: k\neq j}{-y_k \frac{\partial J}{\partial y_k}}) \\
&= y_j(\frac{\partial J}{\partial y_j} - \sum_{k}{y_k \frac{\partial J}{\partial y_k}}) \\
&= y_j(\frac{\partial J}{\partial y_j} - \langle \mathbf{y}, \frac{\partial J}{\partial \mathbf{y}} \rangle)
\end{split}
\end{equation} %]]></script>

<p>Finally, we reach equation \eqref{eq:error_u_res}.</p>

<p>In this way, softmax operation can be implemented to be a standalone layer.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Strictly speaking, this converts overflow into underflow. 
  Underflow is no problem, because that rounds off to zero, which is a well-behaved floating point number.
  otherwise, it will be Infinity or NaN. see <a href="http://lingpipe-blog.com/2009/03/17/softmax-without-overflow/">this article</a> for details. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Wantee Wang</span></span>

      




<time class='entry-date' datetime='2015-02-05T21:08:17+08:00'><span class='date'><span class='date-month'>Feb</span> <span class='date-day'>5</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>9:08 pm</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/neural-network/'>neural network</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://wantee.github.io/blog/2015/02/05/rnnlib-softmax-layer/" data-via="" data-counturl="http://wantee.github.io/blog/2015/02/05/rnnlib-softmax-layer/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2015/02/05/rnnlib-introduction/" title="Previous Post: RNNLIB: Introduction">&laquo; RNNLIB: Introduction</a>
      
      
        <a class="basic-alignment right" href="/blog/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/" title="Next Post: RNNLIB: Connectionist Temporal Classification and Transcription Layer">RNNLIB: Connectionist Temporal Classification and Transcription Layer &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/03/03/dropout-training/">Dropout Training</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/08/rnnlib-connectionist-temporal-classification-and-transcription-layer/">RNNLIB: Connectionist Temporal Classification and Transcription Layer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/05/rnnlib-softmax-layer/">RNNLIB: Softmax Layer</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/05/rnnlib-introduction/">RNNLIB: Introduction</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/wantee">@wantee</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'wantee',
            count: 0,
            skip_forks: true,
            skip_repos: [ "wantee.github.io" ],
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Wantee Wang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'wantee';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://wantee.github.io/blog/2015/02/05/rnnlib-softmax-layer/';
        var disqus_url = 'http://wantee.github.io/blog/2015/02/05/rnnlib-softmax-layer/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>






  <!-- mathjax config similar to math.stackexchange -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'] ],
      displayMath: [ ['$$', '$$']],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
    TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
  </script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</body>
</html>
