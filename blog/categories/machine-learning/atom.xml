<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine Learning | Formula Coding]]></title>
  <link href="http://wantee.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://wantee.github.io/"/>
  <updated>2015-04-27T20:22:48+08:00</updated>
  <id>http://wantee.github.io/</id>
  <author>
    <name><![CDATA[Wantee Wang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The XOR Problem]]></title>
    <link href="http://wantee.github.io/blog/2015/04/27/the-xor-problem/"/>
    <updated>2015-04-27T10:39:02+08:00</updated>
    <id>http://wantee.github.io/blog/2015/04/27/the-xor-problem</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#probelm">Probelm</a></li>
  <li><a href="#non-linear-boundary">Non-linear Boundary</a></li>
  <li><a href="#removing-redundant-features">Removing Redundant Features</a></li>
</ul>

<p>The XOR is an interesting problem, not only because it is a classical example for <em>Linear Separability</em>, but also it played a significant role in the history of neutral network research.</p>

<h2 id="probelm">Probelm</h2>

<p>The truth table for XOR is</p>

<table>
  <thead>
    <tr>
      <th>x</th>
      <th>y</th>
      <th>x <em>xor</em> y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>It is impossible for a classifier with linear decision boundary to learn an XOR function. This can be seen easily by the following plot.</p>

<p><img class="center" src="/images/posts/xor.png" title="“The XOR Problem” “fig:xor”" ></p>

<p>Apparently, we can’t using a line to separate the two classes.</p>

<h2 id="non-linear-boundary">Non-linear Boundary</h2>

<p>If we take a carefully look at the scatter figure, it can be found that it’s easy to use an ellipse or hyperbola to separate the classes.</p>

<p>Recall that, the general equation for ellipse or hyperbola is</p>

<script type="math/tex; mode=display">\begin{equation}\label{eq:ellipse}
Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0
\end{equation}</script>

<p>Thus, we can just feed those above components($x, y, x^2, y^2, xy$) to a linear classifier, and see if the classes can be separated.</p>

<p>We use <a href="http://scikit-learn.org/">scikit-learn</a> to perform the experiments. Following shows the code,</p>

<p><script src="https://gist.github.com/1c74b8336494cb0e9c6d.js?file=xor-5d.py"> </script></p>

<p>After running, we see the final decision boundary,</p>

<p><img class="center" src="/images/posts/xor-5d.png" title="“Non-linear boundary” “fig:xor-5d”" ></p>

<h2 id="removing-redundant-features">Removing Redundant Features</h2>

<p>It is can be seen that in the final equation \eqref{eq:ellipse}, $A = D$ and $C = E$(it will be more clear if we use logistic regression to fit the data). Actually, for boolean features, the high-order polynomial features are useless, because $\forall n, x_i^n = x_i$. So we can only use the interaction features($x_ix_j$). This time we get the features from <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures">PolynomialFeatures</a> class of scikit-learn. </p>

<p><script src="https://gist.github.com/1c74b8336494cb0e9c6d.js?file=xor-3d.py"> </script></p>

<p>Again, we show the final decision boundary,</p>

<p><img class="center" src="/images/posts/xor-3d.png" title="“Non-linear boundary using 3-d features” “fig:xor-3d”" ></p>

<p>By adding polynomial features to the model inputs, we are actually mapping the features to higher dimension. This is the SVM’s job, here we just choose the features manually.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Discriminative vs Generative]]></title>
    <link href="http://wantee.github.io/blog/2015/03/22/discriminative-vs-generative/"/>
    <updated>2015-03-22T22:44:20+08:00</updated>
    <id>http://wantee.github.io/blog/2015/03/22/discriminative-vs-generative</id>
    <content type="html"><![CDATA[<p>Models in Machine Learning can often be divided into two main categories, <em>Generative</em> and <em>Discriminative</em>.
The fundamental difference between them is:</p>

<ul>
  <li>Discriminative models learn the (hard or soft) boundary between classes</li>
  <li>Generative models model the distribution of individual classes</li>
</ul>

<p>In mathematics, discriminative models directly estimate posterior probabilities $P(y\mathop{|}x)$, while generative models model class-conditional pdfs $p(x\mathop{|}y)$ and prior probabilities $P(y)$, therefore the joint probability distributions $p(x,y)$.</p>

<p>Generative models often make some assumption on the underlying probability distributions and model it. Thus it is can be used to generate new samples from the learned distribution.</p>

<p>A simple way to distinct the two models is by considered the examples used during training. Generative model only needs examples of a particular class which it modelling. However, Discriminative model needs examples of at least two classes to find the boundary. </p>

<h2 id="examples">Examples</h2>

<p>Some models can be seen as generative-discriminative pairs, e.g.,</p>

<ul>
  <li>Classifiers: Naive Bayes and Logistic Regression</li>
  <li>Sequential Data: HMM and CRF</li>
</ul>

<p>Neutral networks are discriminative model because they compute $p(output\mathop{|}input)$.</p>

<h2 id="discriminative-and-generative-training">Discriminative and Generative Training</h2>

<p>Training approaches can also be classified as discriminative or generative. Even though with the same model, we can choose different training approaches.</p>

<p>For example, the HMM-GMM model used in speech recognition, when we do MLE training with Baum–Welch algorithm, we are using a generative training method. However when we do MPE training, we are using a discriminative training method.</p>

]]></content>
  </entry>
  
</feed>
