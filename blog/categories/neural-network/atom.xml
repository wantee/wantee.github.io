<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Neural Network | Formula Coding]]></title>
  <link href="http://wantee.github.io/blog/categories/neural-network/atom.xml" rel="self"/>
  <link href="http://wantee.github.io/"/>
  <updated>2015-02-05T18:14:36+08:00</updated>
  <id>http://wantee.github.io/</id>
  <author>
    <name><![CDATA[Wantee Wang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[RNNLIB: Introduction]]></title>
    <link href="http://wantee.github.io/blog/2015/02/05/rnnlib-introduction/"/>
    <updated>2015-02-05T16:02:28+08:00</updated>
    <id>http://wantee.github.io/blog/2015/02/05/rnnlib-introduction</id>
    <content type="html"><![CDATA[<p>RNNLIB is a recurrent neural network library for sequence learning problems,
which is written by <a href="http://www.cs.toronto.edu/~graves/">Alex Graves</a>.</p>

<p>In <a href="http://www6.in.tum.de/pub/Main/Publications/Graves2006a.pdf">this paper</a>,
Graves proposed the CTC(Connectionist Temporal Classification), 
which allows the system to transcribe unsegmented sequence data. 
The most exciting thing is that by training a deep bidirectional 
LSTM network with CTC, it is possible to 
perform automatic speech recognition in 
an <a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf">end-to-end fashion</a>, 
i.e. without any human expertise.</p>

<p>RNNLIB covers all the theories in Gravesâ€™s paper, including:</p>

<ul>
  <li>Bidirectional Long Short-Term Memory</li>
  <li>Connectionist Temporal Classification</li>
  <li>Multidimensional Recurrent Neural Networks</li>
</ul>

<p>I will try to explain the codes in RNNLIB in following posts.</p>

]]></content>
  </entry>
  
</feed>
